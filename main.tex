% !TeX root = main.tex
\input{template/layout.tex}
\input{template/math.tex}
\input{course_specific.tex}





\title{Mathematics of Reinforcement Learning}

\author{Notes by Moritz Roos}
\date{}
%remove page number from tableofcontents page
%\AtBeginDocument{\addtocontents{toc}{\protect\thispagestyle{empty}}}
\begin{document}
\maketitle

\tableofcontents
%

\clearpage
\section{Introduction}
\textbf{Goals:}
\begin{itemize}
    \item Look at different types of problems.
    \item Learn the basic principles of Reinforcement Learning (RL).
    \item Lean when to apply RL and when not to?
\end{itemize}

\subsection{It's-a me, Mario!}
We want to fix some terminology by the example of the Super Mario game.
\begin{itemize}
    \item \emph{Agent:} player
    \item \emph{Environment:} game
    \item \emph{state:} the frame/screen being presented to the agent
    \item \emph{Action:} input on the controller
    \item \emph{policy:} a correspondence between states and actions
    \item \emph{episode:} one run of the game from start to finish
    \item \emph{reward:} feedback mechanism telling you how good/bad an action performs in a given state
    \item \emph{return:} criterion to be optimized, typically cumulative (discounted, expected) rewards
    \item \emph{state dynamics:} how the next state is obtained from the current state and the current action
\end{itemize}


\begin{figure}[ht]
    \centering
    %\caption{}
    %\incsvg{path/}{path/file}
    \incsvg{figures}{figures/examplemodell}
    \label{fig:examplemodell}
\end{figure}




\section{Markov decision processes}

\textbf{Goals:}
\begin{itemize}
    \item Develop a mathematical framework for dynamic decision making problems under uncertainty.
    \item Lean how to solve these problems if the model is known.
\end{itemize}

\subsection{Definition of markov decision processes}
We look for stochastic processes (a family of random variables indexed by time) \( \{S_n\}_{n \in  \mathbb{N}_0 }, \{A_n\}_{n \in \mathbb{N}_{0}} \{R_n\}_{n \in \mathbb{N}_{0}} \) modelling the dynamic evolution of states, actions and rewards.

\begin{definition}[Markov decision model]
    A \emph{Markov Decision Model} is a tuple \( (S, A, D, p, r ,\gamma) \) consisting of the following components:
    \begin{enumerate}
        \item a finite set \( S \)  called \emph{state space}.
        \item a finite set \( A \) called \emph{action space}
        \item a set \( D \subseteq S \times A \) whose elements are the \emph{admissible state-action pairs}
        \item a \emph{transition probability function} \( p: S \times S \times A \implies [0,1] \)
              \[
                  (s', s, a) \mapsto p(s' | s, a)\note{This is the probability of ending up in \( s' \) when performing action \( a \) in state \( s \). \( s' \mapsto p(s'| s,a) \) is a probability mass function.}
              \] satisfying
              \[
                  \sum_{s' \in S}p(s' | s, a) = 1
              \] for all \( (s,a) \in S \times A \).
        \item a \emph{reward function} \( r: D \to \mathbb{R} \)
              \[
                  (s,a) \mapsto r(s,a)\note{The reward for performing action \( a \)  in state \( s \).}
              \]
        \item a \emph{discount factor} \( \gamma \in (0,1] \).\note{This encodes the time-value of rewards. \( \gamma^{-1}\cdot r(s,a) \) is the value at time \( 0 \) of receiving \( r(s,a) \) \( n \) steps into the future.}

    \end{enumerate}
\end{definition}


\begin{definition}[Policy \qquad \( \Pi \)  \qquad \( \Pi_d \) ]\label{def_policy}
    A \emph{policy} is a mapping \( \pi: S \times A \to [0,1] \)
    \[
        (s,a) \mapsto \pi(a|s)
    \]
    such that
    \[
        \sum_{a \in A} \pi(a|s) = 1 \note{This means that \( a \mapsto \pi(a|s) \) is a probability mass function for each fixed \( s \) . Thus one action \textit{has} to be chosen.}
    \] for all \( s \in S \) and
    \[
        \pi(a|s) = 0
    \] for all \( s,a \in S\times A \setminus D\).

    We say that \( \pi \) is a \emph{deterministic policy} if
    \[
        \fa{s \in S:}\ex{a \in A:}\pi(a|s) = 1.
    \] We write \( \Pi \) and \( \Pi_d \) for the set of all policies and the subset of deterministic policies respectively.

\end{definition}

To wit, \( \pi(a|s) \) is interpreted as the probability of choosing action \( a \) in state \( s \). One could also introduce \quotes{non-Markovian} policies which depend on the entire history of states and actions or \quotes{non-stationary} policies which depend on the current state and current time.
However, we spare ourselves the trouble, since we will eventually see that there always is a \quotes{stationary, Markovian} optimal policy as in \cref{def_policy}.


\ \\
Intuitively, what happens for a given MDM and a policy \( \Pi \) is the following:

\begin{itemize}
    \item Start in an initial state \( S_{0}:= s_{0} \).
    \item Using the policy \( \pi \), randomly draw an action \( A_{0} \) from \( a \mapsto \pi(a|S_{0}) \).
    \item Collect the reward \( R_{0} := r(S_{0}, A_{0})\) and draw the next state \( S_{1} \) from \( s' \mapsto p(s'|S_{0}, A_{0}) \).
    \item repeat this procedure to construct \( S_{0}, A_{0}, R_{0}, S_{1}, A_{1}, R_{1},\mydots \)
\end{itemize}

Informally, we can then introduce the objective
\[
    \mathbb{E} \left[ \sum_{n=0}^{\infty} \gamma^{n} \cdot R_n \right] = \mathbb{E} \left[ \sum_{n=0}^{\infty} \gamma^{n} \cdot r(S_{n}, A_{n}) \right].
\]


Now we want to construct the mentioned stochastic processes. They should be consistent with our definition of MDM's and policies.\note{This can be seen as the formalization of the intuitive description of what should happen for a given MDM and a policy.} This enables us to make sense of notions like the expected return mentioned above.

For this, the first ingredient is a probability space. We choose the sample space as 
\[
    \Omega := (S \times A)^\infty = \bigtimes_{n=0}^{\infty} (S \times A)
\]
and let the \( \sigma \)-algebra \( \mathcal{A} \) on \( \Omega \) be the power set \( \mathcal{P}(\Omega) \) of \( \Omega \). We can safely do so, since \( \Omega \) is countable. In fact, any element \( w \in \Omega  \) takes the form 
\[
    \omega = (s_{0},a_{0},s_{1},a_{1},s_{2},a_{2},\mydots)
\]  
where \(  \left\{ s_n \right\}_{n \in \mathbb{N}_{0}} \subseteq S, \left\{ a_n \right\}_{n \in \mathbb{N}_{0}} \subseteq A \). With that in mind, we define functions
\[
    S_n(\omega) = S_n((s_{0},a_{0},s_{1},a_{1},\mydots)) := s_n
\]
\[
    A_n(\omega) = A_n((s_{0},a_{0},s_{1},a_{1},\mydots)) := a_n  
\]
for \( n \in \mathbb{N}_{0}, \omega \in \Omega \). 

Then \( S_n, A_n \) trivially\note{Trivially, since we chose the power set as \( \sigma \)-algebra. } are measurable functions ( = random variables) taking values in \( S, A \) respectively.  What is left is showing that there exists a probability measure \( \mathbb{P}^\pi_\mu \) on \( (\Omega, \mathcal{A}) \) such that 
\( S_{0},A_{0}, S_{1},A_{1},\mydots \) have the desired distribution.\note{The \( A \) 's should be determined by the policy and the last state. The \( S \) 's by the transition function and the taken action.}
This can be constructed from \( p, \pi \)  and an initial distribution \( \mu \) on \( S \) such that \( S_{0} \sim \mu \).

What we would like to do is define
\begin{align*}
    \mathbb{P}_\mu^\pi [\left\{ \omega \right\}] &= \mathbb{P}_\mu^\pi[{(s_{0},a_{0},s_{1},a_{1},s_{2},a_{2},\mydots)}] \\
    &:= \mu[\left\{ s_{0} \right\}] \cdot \pi(a_{0} | s_{0}) \cdot p(s_{1} | s_{0},a_{0}) \cdot \pi(a_{1} | s_{1}) \cdot p(s_{2}|s_{1},a_{1}) \cdot \pi(a_{2}|s_{2}) \cdot \mydots \\
    &= \mu[\left\{ s_{0} \right\}] \pi(a_{0},s_{0}) \cdot \prod_{n=1}^\infty p(s_n | s_{n-1}, a_{n-1}) \pi(a_n | s_n) \\
\end{align*}
for all \( \omega = (s_{0},a_{0},s_{1},a_{1},\mydots) \in \Omega \).
The problem is, however, that the resulting probability is usually zero, as this is an infinite product with factors valued in \( \left[ 0,1 \right] \).

\begin{theorem}[Ionescu-Tulcea for MDMs]
    \label{Ionescu-Tulcea}
    Let \( S,A,D,p,r,\gamma \) be an MDM, \( \pi \) a policy, and \( \mu \) a probability measure on \( S \). Then there exists a unique probability measure \( \mathbb{P}_\mu^\pi \) on \( \Omega, \mathcal{A} \) with the property 
    
    \begin{align*}\label{Ionescu-Tulcea-eq}\tag{\textasteriskcentered}
        \quad &\mathbb{P}_\mu^\pi[B \times \bigtimes_{k = n+1}^{\infty} (S \times A)] \\
         &:= \sum_{(s_{0},a_{0},\mydots,s_n,a_n) \in B} \mu[{s_{0}}] \cdot \pi(a_{0}|s_{0}) \cdot \prod_{k=1}^n p(s_k | s_{k-1}, a_{k-1}) \cdot \pi(a_{k} | s_{k})\\
    \end{align*}



    for all \( B \subseteq (S \times A)^{n+1}, n \in \mathbb{N}_0 \). In particular, it holds that 
    \begin{align*}\label{Ionescu-Tulcea-eq-2}\tag{\textasteriskcentered\textasteriskcentered}
        \quad &\mathbb{P}_\mu^\pi [S_{0}=s_{0}, A_{0} = a_{0}, \mydots, S_n = s_n, A_n = a_n] \\
        &= \mu[{s_{0}}] \cdot \pi(a_{0}|s_{0})\cdot \prod_{k=1}^{n} p(s_k | s_{k-1}, a_{k-1}) \cdot \pi(a_k | s_k) \\
    \end{align*} 

    for all \( (s_{0},a_{0},s_{1},a_{1},\mydots,s_n, a_n) \in (S \times A)^{n+1}\) and \( n \in \mathbb{N}_{0} \).  \note{This doesnt yet mean that this \( \mathbb{P}_\mu^\pi \) agrees with \( p \) and \( \pi \). But it will be a consequence later.}
\end{theorem}


\begin{proof}[Proof (idea)]
    The key idea is to take \eqref{Ionescu-Tulcea-eq} as a definition and extend it to the full \( \sigma \)-Algebra. Note that the sets of the form 
    \[
        B \times \bigtimes_{k=n+1}^{\infty} (S \times A) 
    \] 
    are an intersection stable ring of sets generating \( \mathcal{A} = \mathcal{P}(\Omega) \). Moreover, it is not hard to show that \( \mathbb{P}_\mu^\pi \) defined by \eqref{Ionescu-Tulcea-eq} is a well-defined additive set function.
    
    With a bit of work, one can show that \( \mathbb{P}_\mu^\pi \) is even \( \sigma \)-additive, hence it is a pre-measure. The existence and uniqueness of \( \mathbb{P}_\mu^\pi \) on all of \( \mathcal{A} \) is thus a direct consequence of Caratheodory's extension theorem.  
    
    Finally, note that 
    \begin{align*}
       &\left\{ S_{0}=s_{0}, A_{0},a_{0},\mydots,S_n = s_n, A_n = a_n \right\} \\
        &= \left\{ (s_{0},a_{0},a_{1},a_{1},\mydots,s_n,a_n) \right\} \times \bigtimes_{k=n+1}^{\infty}(S \times A),\\
    \end{align*}

    so 
    \begin{align*}
        &\mathbb{P}_\mu^\pi \left[ S_{0}=s_{0}, A_{0},a_{0},\mydots,S_n = a_n  \right] \\
        &\overset{\eqref{Ionescu-Tulcea-eq}}{=} \mu[\left\{ s_{0} \right\}] \pi(a_{0},s_{0}) \prod_{k=1}^{n} p(s_k | s_{k-1}, a_{k-1}) \pi(a_{k}|s_{k}).
    \end{align*}
\end{proof}


Having constructed \( \mathbb{P}_\mu^\pi \), it is now straightforward to show that \( \left\{ S_n \right\}_{n \in \mathbb{N}_{0}}, \left\{ A_n \right\}_{n \in \mathbb{N}_{0}} \) have the desired properties under this measure.

\begin{corollary}[Properties of \( \mathbb{P}_\mu^\pi \) ]
    In the setting of \cref{Ionescu-Tulcea}, it holds that
    \begin{enumerate}
        \item \( \mathbb{P}_\mu^\pi[S_{0} = s] = \mu[\left\{ s \right\}], s \in S \)
        \item \( \mathbb{P}_\mu^\pi [S_{n+1} = s' \,|\, S_n = s, A_n = a] = p(s' \,|\, s,a), n \in \mathbb{N}_{0}, s',s \in S, a \in A \)
        \item  \( \mathbb{P}_\mu^\pi[A_{n} = a \,|\, S_{n} = s] = \pi(a | s) \)
        \item \( \begin{aligned}[t]\note{This is the Markov property of the state action pair evolution.}
             & \mathbb{P}_\mu^\pi [ S_{n+1} = s_{n+1}, A_{n+1} = a_{n+1} \,|\, S_{0} = s_{0}, A_{0} = a_{0}, \mydots, S_n = s_n, A_n = a_n] \\
             &\quad =  \mathbb{P}_{\mu}^\pi[S_{n+1} = s_{n+1}, A_{n+1} = a_{n+1} \,|\, S_n = s_n, A_n = a_n]\\[0.2em]
        \end{aligned} \)   for all \( (s_{0},a_{0},\mydots,s_{n+1}, a_{n+1}) \in (S \times A)^{n+2}, n \in \mathbb{N}_{0} \).
        \item \( \mathbb{P}_\mu^\pi[S_{n+1} = s_{n+1} \,|\, S_{0} = s_{0},\mydots, S_n = s_n] = \mathbb{P}_\mu^\pi[ S_{n+1} = s_n \,|\, S_n = s_n]\) 
        for all \( s_{0},s_{1},\mydots,s_{n+1} \in S^{n+1}, n \in \mathbb{N}_{0}\) if \( \pi \) is deterministic \note{That is, if for each \( s \in S \), there exists \( a \in A \) such that \( \pi(a | s) = 1 \).}.  
    \end{enumerate}
\end{corollary}


\begin{proof}[Proof of (ii)]
    
It holds that
\begin{align*}
    &\mathbb{P}_\mu^\pi [S_{n+1} = s', S_n = s, A_n = a] \\
    &= \sum_{a' \in A} \mathbb{P}_\mu^\pi[ S_{n+1} = s', A_{n+1} = a', S_{n} = s, A_{n} = a]\\
    &\overset{\text{\cref{Ionescu-Tulcea}}}{=} \sum_{a' \in A} \sum_{(s_{0},a_{0}),\mydots,(s_{n-1},\mydots,a_{n-1}) \in S \times A} \bigg( \bigg. \mu[\left\{ s_{0} \right\}] \pi(a_{0}| s_{0}) \prod_{k=1}^{n-1} p(s_k | s_{k-1}, a_{k-1}) \pi(a_{k} | s_{k}) \\
    & \cdot p(s | s_{n-1}, a_{n-1}) \pi(a | s) p(s' | s,a) \pi(a' | s')\bigg.\bigg)\\
    &= p(s' | s, a) \sum_{a' \in A} \pi(a' | s') \cdot \sum_{(s_{0},a_{0}),\mydots,(s_{n-1},a_{n-1})} \mu[\left\{ s_{0} \right\}] \pi(a_{0},s_{0}) \prod_{k=1}^{n-1} p(s_{k} | s_{k-1}, a_{k-1}) \pi(a_{k} | s_{k}) p(s | s_{n-1}, a_{n-1}) \pi(a | s) \\
    &= p(s' | s, a) \sum_{a' \in A} \pi(a' | s) \cdot \mathbb{P}_\mu^\pi [S_n = s, A_n = a] \\
    &= p(s' | s, a) \cdot \mathbb{P}_\mu^\pi [ S_n = s , A_n = a].
\end{align*}

Thus 
\begin{align*}
    \mathbb{P}_\mu^\pi &[ S_{n+1} = s' \,|\, S_n = s , A_n = a]\\
    &= \frac{\mathbb{P}_{\mu}^\pi [S_{n+1} = s, S_n =s ,A_n = a]}{\mathbb{P}_\mu^\pi[ S_n = s, A_n = a]}\\
    &= p(s' | s, a).
\end{align*}

The rest follows by similar arguments.

\end{proof}


Given \( s \in S \), we write \( \delta_s \) for the Dirac measure with mass on \( s \), that is 
\[
    \delta_s[\left\{ s \right\}] = 1, \quad \delta_s[\left\{ s' \right\}] = 0, s' \in S \setminus \left\{ s \right\}
\]
For any \( \pi \in \Pi  \), we then write \( \mathbb{P}_s^\pi := \mathbb{P}_{\delta_s}^\pi\) for the expectation operator with respect to \( \mathbb{P}_\mu^\pi \) and let \( \mathbb{E}_s^\pi = \mathbb{E}_{\delta_s}^\pi \).
 


\subsection{Value Functions and bellman Equations}

\begin{definition}[\( V^\pi \qquad V\) ]
    For any \( \pi \in \Pi \), the function \( V^\pi: S \to \mathbb{R} \),
    \[ s \mapsto V^\pi(s) := \mathbb{E}_s^\pi\left[ \sum_{n=0}^{\infty}\gamma^n r(S_n, A_n) \right] \]
    is called the \emph{state value function} (if it exists).
    Moreover, if it exists, the function \( V: S \to \mathbb{R} \), 
    \[
        s \mapsto V(s) := \sup_{\pi \in \Pi} V^\pi (s) = \sup_{\pi \in \Pi}\, \mathbb{E}_{s}^{\pi}\left[ \sum_{n=0}^{\infty} \gamma^n r(S_n, A_n) \right]
    \] 
    is called \emph{(optimal) value function}.

    Finally, a policy \( \pi^\star \in \Pi \) such that \( V^{\pi^\star}(s) := V(s) \) is called \emph{optimal} for the initial state \( s \in S \). 

\end{definition}

\begin{remark}
    \( V^\pi \) and \( V \) exist under very mild conditions indeed. Since \( S \) and \( A \) are finite, we conclude that \( |r| \) is bounded. A sufficient condition for the existence of \( V^\pi \) and \( V \) is hence \( \gamma < 1  \) as 
    \[
        \sum_{n=0}^{\infty} \gamma^n | r(S_n, A_n)| \leq \max_{(s,a) \in D} | r(a,s) | \sum_{n=0}^{\infty} \gamma^n = \frac{1}{1-\gamma} \max_{(s,a) \in D} |r(s,a)| \text{a.s.}
    \]        
    for all \( \mathbb{P}_s^\pi \). If the MDM has a finite time horizon, meaning that there is \( N \in \mathbb{N}_{0} \) s.t. \( r(S_n, A_n) = 0 \fa{n \geq N}  \mathbb{P}_s^\pi-\text{a.s.}\) for all \( s \in S,  \in \Pi \), we do not have to assume \( \gamma < 1 \).

\end{remark}

\emph{Standing Assumption:} There exists some \( C > 0 \) s.t. 
\[
    \sum_{n=0}^{\infty} \gamma^n | r(S_n, A_n)| \leq C\quad  \mathbb{P}_s^\pi-\text{a.s.}
\] 
for all \(  \in \Pi, s \in S\). 
While it may seem odd that we are now working with an entire family of optimization problems \( \left\{ V(s) \right\}_{s \in S} \), it is not hard to imagine, that these problems are intimately related and we can learn somethig from that connection.

\begin{lemma}[Time shift]
    \label{time_shift_lemma}
    Let \( s,s' \in S, a \in A \), and \( \pi \in \Pi \).
    Define 
    \[
        \mathbb{P}_{s,a,s'}^\pi := \mathbb{P}_s^\pi[ \cdot \,|\, S_{0} = s, A_{0} = a, S_{1} = s']
    \] 
    and 
    \[
        \hat{S_n} := S_{n+1}, \hat{A}_n := A_{n+1}, n \in \mathbb{N}_{0}
    \]
    then the distribution of \( \left\{ (\hat{S}_n, \hat{A}_n) \right\}_{n \in \mathbb{N}_{0}} \) under \( \mathbb{P}_{s,a,s'}^\pi \) is the same as the distribution of 
    \( \left\{ (S_n, A_n) \right\}_{n \in \mathbb{N}_{0}} \) under \( \mathbb{P}_{s'}^\pi \).    
\end{lemma}

\begin{proof}
    For \(  n  \in \mathbb{N}_{0}, (s_{0},a_{0},s_{1},a_{0},\mydots,s_n,a_n) \in (S\times A)^{n+1} \), it suffices to show

    \[
        \mathbb{P}_{s,a,s'}^\pi [ \hat{S}_0 = s_{0}, \hat{A}_{0} = a_{0}, \mydots, \hat{S}-n = s_n, \hat{A}_n = a_n] = \mathbb{P}_{s'}^\pi [S_0 = s_{0}, A_{0} = a_{0}, \mydots, S_n = s_n, A_n = a_n].
    \]
    The identity is immediate from the definitions, if \( s_{0} \neq s' \) since both sides are zero in that case. So assume further that \( s_{0} = s' \).  
    Then we have

    \begin{align*}
        &\mathbb{P}_{s,a,s'}^\pi[\hat{S}_0 = s', \hat{A}_{0} = a_{0}, \mydots, \hat{S}_n = s_n, \hat{A}_n = a_n] \\
        &= \mathbb{P}_s^\pi[ \hat{S}_{0} = s', \hat{A}_{0} = a_{0}, \mydots, \hat{S}_{0} = s_n, \hat{A}_n = a_n \,|\, S_{0} = s, A_{0} = a, S_{1} = s'] \\
        &= \mathbb{P}_s^\pi [S_{0} =s, A_{0}= a, S_1 = s', A_{1} = a_{0}, \mydots, S_{n+1} = s_n, A_{n+1} = a_n] \,/\, \mathbb{P}_s^\pi[S_0 = s, A_{0} = a, s_n = s']\\
        &= \frac{\delta_s[\left\{ s \right\}] \pi(a|s) p(s' | s,a) \pi(a_{0}|s') \prod_{k=1}^n p(s_k | s_{k-1}, a_{k-1}) \pi(a_k | s_k)}{\delta_s[\left\{ s \right\}] \pi(a | s) p(s' | s,a)} \\
        &= \delta_{s'}[\left\{ s' \right\}] \pi(a_{0} | s') \prod_{k=1}^n p(s_k | s_{k-1}, a_{k-1}) \pi(a_k, s_k) \\
        &= \mathbb{P}_{s'}^\pi[ S_{0} = s', A_{0} = a_{0}, \mydots, S_n = s_n, A_n = a_n]\\ 
    \end{align*}
\end{proof}


\begin{theorem}[Bellman Equation]
    Let \( s \in S, \pi   \). Then 
    \begin{align*}
                V^\pi(s) &= \sum_{s' \in S, a \in A} \left[ r(s,a) + \gamma V^\pi(s')  \right] \pi(a | s) p(s' | s,a)\\
                 &= \mathbb{E}_s^\pi \left[ r(S_{0},A_{0}) + \gamma V^\pi(s_{1}) \right] \\
                 \bigg(&= \sum_{a \in A} \left[ r(s,a) + \gamma \sum_{s' \in S} p(s' | s,a) V^\pi(s')\right] \pi(a | s)\bigg) \note{The part in square brackets will get its own name in the next definition.}
    \end{align*}
\end{theorem}
\begin{proof}
    We have 
    \begin{align*}
        V^\pi(s) &= \mathbb{E}_s^\pi \left[  \sum_{n=0}^{\infty}\gamma^n r(S_n, A_n) \right] \\
        &= \sum_{s' \in S, a\in A} \mathbb{E}_s^\pi \left[ r(S_{0},A_{0}) + \gamma\cdot \sum_{n=1}^\infty \gamma^{n-1} r(S_n, A_n)    \,|\, S_{0} = s, A_{0} = a, S_{1} = s' \right] \cdot \mathbb{P}_s \left[ S_{0} = s, A_{0} = a, S_{1} = s' \right]\\
        &= \sum_{s' \in s, a \in A} \left[ r(s,a) + \gamma \mathbb{E}_s^\pi \left[ \sum_{n=1}^\infty \gamma^{n-1} r(S_n, A_n) | S = s, A_{0} = a, S_{1} = s' \right] \right] \pi(a|s) p(s'| s,a) \\
        &= \sum_{s' \in S, a \in A }\left[ r(s,a) + \gamma \mathbb{E}_{s'}^\pi \left[ \sum_{n=0}^{\infty} \gamma^n r(S_n, A_n) \right] \right] \\
        &= \sum_{s' \in S, a \in A} \left[ r(s,a) + \gamma V^\pi(s') \right] \pi(a| s) p(s' | s,a)
    \end{align*}
    where \( \sum_{n=1}^{\infty} \gamma^{n-1} r(S_n, A_n)  = \sum_{n=0}^{\infty} \gamma^n r(\hat{S}_n, \hat{A}_n) \sim \sum_{n=0}^{\infty} \gamma^n r(S_n, A_n)\) under \( \mathbb{P}_{s'}^\pi \) by \cref{time_shift_lemma}.  
\end{proof}


\begin{definition}[\( Q^\pi \) \qquad \( Q \) ]
    For any \( \pi \in \Pi \), we refer to \( Q^\pi: S \times A \to \mathbb{R} \),
    \[
        (s,a) \mapsto Q^\pi(s|a) := r(s,a) + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot V^\pi(s')
    \]  
    as the \emph{action value function} for the policy \( \pi \). \\
    
    Similarly we call \( Q: S \times A \to R \), 
    \[  
        (s,a) \mapsto Q(s|a) := r(s,a) + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot V(s')
    \]
    the \emph{optimal action value function}.
\end{definition}

Clearly
\[
    V^\pi(s) = \sum_{a \in A} Q^\pi (s|a) \cdot \pi(a|s) 
\]
and we \textit{expect} 
\[
    V(s) = \max_{a \in A(s)} Q(s|a),
\]
where 
\(  A(s) := \left\{  a \in A: (s,a) \in D \right\} \) are the admissible actions in state \( s \in S\).

Since 
\[  
    V^\pi(s) = \sum_{s' \in S, a \in A} \left[ r(s,a) + \gamma \cdot V^\pi(s') \right] \pi(a|s) \cdot p(s' | s,a)
\] for \( s \in S \) we see that \( V^\pi \) can be computed by solving a linear system of equations\note{This really is feasible in practice, if the state space is small enough!}. 

Similarly, we have
\[  
    Q^\pi(s|a) = r(s,a) + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot \sum_{a' \in A} \pi(a' | s') \cdot Q^\pi (s' | a') 
\]
for all \( (s,a) \in S \times A \). 
If \( \pi^* \) is optimal, it clearly also satisfies the Bellman equation. 
It is therefore not surprising, that there is a variant of the Bellman equation for the optimal value function \( V \).
For any \( \pi \in \Pi \), we have 

\begin{align*}
      V^\pi(s) &= \sum_{a' \in A(s)} \left[ r(s, a') + \gamma \cdot \sum_{s' \in S}p(s'| s, a') \cdot V^\pi(s') \right] \cdot \pi(a' | s) \note{If \( a' \notin A(s) \) then \( \pi(a') = 0 \). }   \\
      &\leq \max_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{ s' \in S} p(s' | s,a) \cdot V^\pi(s') \right] \cdot \sum_{a' \in A(s)} \pi(a' | s)\\
      &\leq \max_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{ s' \in S} p(s' | s,a) \cdot V(s') \right] \cdot \sum_{a' \in A(s)} \pi(a' | s)\\
      &= \max_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{ s' \in S} p(s' | s,a) \cdot V(s') \right] \cdot 1
\end{align*}
for any \( \pi \).
So also 
\[
    V(s) = \max_{\pi \in \Pi} V^\pi(s) \leq  \max_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{ s' \in S} p(s' | s,a) \cdot V(s') \right].
\]

The reverse inequality will now only be derived heuristically:
Suppose that we allow non-stationary policies \( \pi_a^\epsilon \) which first choose a fixed action \( a \in A(s) \) and for any \( s' \in S \) and fixed \( \epsilon > 0 \) follow an \( \epsilon \)-optimal policy \( \pi^{s'} \), meaning \( V^{\pi^{s'}} \geq V(s') - \epsilon \).
The Bellman equation then suggests that \note{We haven't actually proven that it also holds for non-stationary policies.} 
\begin{align*}
     V(s) &\geq V^{\pi_a^\epsilon}(s)\\
          &= \sum_{a' \in A} \left[ r(s, a') + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot V^{\pi_a^\epsilon}(s) \right] \cdot \pi(a'|s) 
          \note{\( V^{\pi_a^\epsilon}(s') = V^{\pi^{s'}}(s') \) and \( \pi(a'|s) = 1 \) iff \( a' = a \)   }\\
          &= r(s,a) + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot V^{\pi^{s'}}(s') \\
          &\geq r(s,a) + \gamma \cdot \sum_{s' \in S} p(s'|s,a) \left[ V(s')-\epsilon \right]
\end{align*}
for arbitrary \( a \in A(s)  \) and \( \epsilon > 0 \).  
so
\[
    V(s) \geq \max_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{s' \in S} p(s'|s,a) \cdot V(s') \right].
\]









\begin{theorem}[Bellman Optimality Equation]
Let \( W: S \to \mathbb{R}  \) satisfy the \emph{Bellman optimality equation},
i.e. 
\[
    W(s) = \max_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{s' \in S} p(s'|s,a) \cdot W(s') \right].
\]
Let moreover \( \pi^* \in \Pi \) be a policy with the property
\[
    \pi^*(a|s) > 0 \implies a \in \argmax_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot W(s') \right].
\]
If in addition 
\[
    \lim_{n \to \infty} \gamma^n W(S_n) = 0 \,\, \mathbb{P}_s^{\pi^*}\text{-a.s.} \fa{s \in S}
\]
and 
\[
    \limsup_{n \to \infty} \gamma^n W(S_n) \geq 0 \,\, \mathbb{P}_s^\pi\text{-a.s.} \fa{s \in S, \pi \in \Pi}, \]
    then \( \pi^* \) is optimal for \textit{all} initial states \( s \in S \) and \( W = V \).   
\end{theorem}
\begin{proof}
For any \( s \in S  \) and \( \pi \in \Pi \), we have 
\begin{align*}
    W(s) &= \max_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{s' \in S} p(s_1 | s, a_{0}) \cdot W(s_{1}) \right] \\
    &\geq \note{Here we have equality if \( \pi = \pi^* \). }\sum_{a_{0} \in A} \left[ r(s, a_{0}) + \gamma \cdot \sum_{s_{1} \in S}p(s_{1} | s,a_{0}) \cdot W(s_{1})  \right] \cdot \pi(a_{0} | s)\\
    &= \sum_{(s_{0},a_{0}) \in S \times A} \left[ r(s_{0},a_{0}) + \gamma \cdot W(s_{1}) \right] \cdot \delta_s[\left\{ s_{0} \right\}] \cdot \pi(a_{0}|s_{0}) \cdot p(s_{1} | s_{0}, a_{0})\note{Now we can use the equation we derived until this point again for \( s = s_{1} \). }\\
    &\geq \sum_{(s_{0},a_{0}) \in S \times A} \left[ r(s_{0},a_{0}) + \gamma \cdot \left( \sum_{a_{1} \in A, s_{2} \in S}(r(s_{1},a_{1}) + \gamma \cdot W(s_2)) \cdot \pi(a_{1}|s_{1}) \cdot p(s_{2}| s_{1},a_{1}) \right) \right] \\
    & \qquad \qquad \qquad \cdot \delta_s[\left\{ s_{0} \right\}] \cdot \pi(a_{0}|s_{0}) \cdot p(s_{1} | s_{0}, a_{0})\\
    &= \sum_{(s_{0},a_{0}),(s_{1},a_{1}) \in S \times A, s_{2} \in S} \left[ r(s_{0},a_{0}) + \gamma \cdot r(s_{1},a_{1}) + \gamma^2 \cdot W(s_2) \right]\\
    &\qquad \qquad \qquad \cdot \delta_s[\left\{  s_{0} \right\}] \cdot \pi(a_{0}|s_{0}) \cdot p(s_{1}| s_{0},a_{0}) \cdot \pi(a_{1} | s_{1}) \cdot p(s_{2} | s_{1}, a_{1})\note{Now we can apply the equation again for \( W(s_2) \) etc\mydots }\\
    &\geq \mydots \\
    &\geq \sum_{(s_{0},a_{0}),\mydots,(s_n,a_n) \in S \times A, s_{n+1} \in S} \left[ \sum_{k=0}^n \gamma^k \cdot r(s_k, a_k) + \gamma^{n+1} \cdot W(s_{n+1}) \right]\\
    &\qquad \qquad \qquad \cdot \delta_s[\left\{  s_{0} \right\}] \cdot \pi(a_{0}|s_{0}) \cdot \prod_{k=1}^n p(s_k | s_{k-1}, a_{k-1}) \cdot \pi(a_{k}| s_{k}) \cdot p(s_{n+1}|s_n, a_n)\\
    &= \mathbb{E}_s^\pi \left[  \sum_{k=0}^{n} \gamma^k \cdot r(S_k, A_k) + \gamma^{n+1} W(S_{n+1}) \right] \\
\end{align*}
which converges against something which by dominated convergence is larger than 
\[
    \mathbb{E}_s^\pi \left[ \sum_{n=0}^{\infty} \gamma^n \cdot r(S_n, A_n) \right] = V^\pi(s).
\]
Similarly for \( \pi = \pi^* \) we get equalities \textit{everywhere}, so \( W(s) = V^{\pi^*}(s). \)
Hence
\[
    V^{\pi^*} = W \geq \sup_{\pi \in \Pi} V^\pi = V \geq V^{\pi^*},
    \]
so 
\[
    W = V = V^{\pi^*},
\]
thus \( \pi^*  \) is optimal for all states \( s \in S \).    
\end{proof}


\begin{theorem}[Existence of a fixed point]
    Assume \( \gamma < 1  \) and define an operator \( \mathcal{T} \) acting on functions 
    \( w: S \to \mathbb{R} \) via 
    \[
        \mathcal{T}[w](s) := \max_{a \in A(s)}\left[ r(s,a) + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot w(s') \right]
    \]
    Then \( \mathcal{T} \) has a unique fixed-point \( W: S \to \mathbb{R} \), that is
    \[
        W(s) = \mathcal{T}[W](s)
    \]
    for all \( s \in S \).\note{That means it solves the bellman optimality equation! This whole Theorem is just a rephrasing of the existence of a solution to a fixed-point problem.}
\end{theorem}
\begin{proof}
    Exercise. Idea: Use Banach fixed-point theorem for the contraction \( \mathcal{T} \) (with rate \( \gamma \)) on some suitable space of functions with some suitable norm.
\end{proof}



With the previous theorem, we now have essentially our first algorithm to solve MDP's numerically:

Starting from an arbitrary initial guess \( w_{0}: S \to \mathbb{R} \) for the value function define for all \( n \in \mathbb{N} \)
\[
    w_n := \mathcal{T}[w_{n-1}].
\]
By Banachs fixed-point theorem, if \( \gamma < 1\) this sequence of functions converges to the value function \( V \), the unique fixed point of \( \mathcal{T} \).
An (approximate) optimal policy is obtained by choosing for each \( s \in S \), \textit{any} maximizer \( a^*(s) \in A(s) \) in \( \mathcal{T}[V](s) \), that is 
\[
    a^*(s) \in \argmax_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{s' \in S} P(s' | s,a) \cdot V(s') \right].
\]

In particular, there always is a \emph{deterministic} optimal policy.














\subsection{Policy improvement and policy Iteration}

The fixed point argument developed in the last section iteratively computes an approximation of \( V \) to solve the MDP.

In this sectionm we instead develop an iterative Method which works on the level of policies.The key to constructing such an iteration is the following theorem.
Standing Assumption for this section:

\[
    \lim_{n \to \infty} \gamma^n V^{\widetilde{\pi}}(S_n) = 0 \,\, \mathbb{P}_s^\pi \text{-a.s.} \text{for all } \pi, \widetilde{\pi} \in \Pi, s \in S
\]


\begin{theorem}[Policy improvement]\label{policy_improvement}
    Let \( \pi, \pi' \in \Pi \) such that for all \( s \in S \) we have
    \begin{equation}
        \sum_{a \in A} \pi(a|s) Q^\pi(s|a) \leq \sum_{a \in A} \pi'(a|s) Q^\pi(s|a). \tag{\textasteriskcentered}\label{policy_improvement_eq}
    \end{equation}
    Then \( \pi' \) outperforms \( \pi \), that is 
    \[
        V^\pi \leq V^{\pi'}.
    \]
    Moreover if the inequality in \eqref{policy_improvement_eq} is strict in some state \( s_{0} \), then \( V^\pi(s_{0}) < V^{\pi'}(s_{0}) \).
\end{theorem}

\begin{remark}
The right hand side of \eqref{policy_improvement_eq} can be interpreted as following the policy \( \pi' \) in the first time step, and then switching to \( \pi \). If this perfoms better than choosing \( \pi \) also for the first step, then \( \pi' \) is better overall.     
\end{remark}


\begin{proof}
    Let \( s \in S \). 
    Have 
    \begin{align*}
        V^\pi(s) &= \sum_{a \in A} \pi(a|s) \cdot Q^\pi(s|a) \\
               &\leq \sum_{a \in A} \pi'(a|s) Q^\pi(s|a) \\
               &=\note{by Definition of \( Q^\pi \)} \sum_{a \in A} \pi'(a|s) \cdot \left[ r(s,a) + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot V^\pi(s') \right]\\
               &= \sum_{(s_{0},a_{0}) \in S \times A, s_{1} \in S} \left[ r(s_{0},a_{0}) + \gamma \cdot V^\pi(s) \right] \cdot \delta_s[s_{0}] \cdot \pi'(a_{0}|s_{0}) \cdot p(s_{1} | s_{0},a_{0}) \\
               &= \mathbb{E}_s^{\pi'} \left[ r(S_{0},A_{0}) + \gamma \cdot V^\pi(S_{1})\right]\\
               &\leq\mydots \note{Now we can use the until now already proven inequality again (\( N -1\) times), but for \( V^\pi(S_{1}) \) instead of for ??} \\
               &\leq \mathbb{E}_s^{\pi'}\left[ \sum_{n=0}^N \gamma^n \cdot r(S_n, A_n) + \gamma^{N+1} \cdot V^\pi(S_{N+1}) \right]              \\ 
    \end{align*}    
    for any \( N \in \mathbb{N} \).
    So 
    \[
        V^\pi(s) \leq \mathbb{E}_s^{\pi'} \left[ \sum_{n=0}^{\infty} \gamma^n \cdot r(S_n, A_n) \right]
    \]
    by limiting both sides and using dominated convergence to exchange limit with Expectation.

    If the inequality in \eqref{policy_improvement_eq} is strict at \( s_{0} \), then all the our first inequality becomes strict. This strictness carries through to the end.
\end{proof}


The previous Theorem gives an easy way of improving a given policy \( \pi \in \Pi \) by choosing \( \pi'  \in \Pi\) s.t. 
\[
    \pi'(a^* | s) = 1 \text{ for some } a^* \in \argmax_{a \in A(s)} Q^\pi(s|a).
\]

This implies
\[
    \sum_{ a \in A}\pi(a|s) Q^\pi(s|a) \leq \sum_{a \in A} \pi'(a|s) \cdot Q^\pi(s|a),
\]

i.e. that \( \pi' \) is better than \( \pi \). We say that a \( \pi' \) chosen this way is \emph{greedy} w.r.t. \( Q^\pi \).
More generally:

\begin{definition}[\( \epsilon-greedy \)]
    Let \( q: S \times A \to \mathbb{R}, \epsilon \in [0,1] \). A policy \( \pi \) is called 

    \begin{enumerate}
        \item \( \epsilon \)\emph{-greedy} w.r.t. \( q \) iff for each \( s \in S \)
        \[
            \pi(a|s) = \begin{cases}
                (1-\epsilon) + \frac{\epsilon}{|A(s)|}, \text{ if } a=a^*(s)\\
                \frac{\epsilon}{|A(s)|} \text{ if } a  A(s) \setminus \left\{ a^*(s) \right\}\\
            \end{cases}
        \]
        for some \( a^*(s) \in \argmax_{ a \in A(s)} \left[ q(s,a) \right] \).
        In case of \( \epsilon> 0 \), we simply speak of \( \pi \) beeing \emph{greedy} w.r.t. q.
        \item \( \epsilon \)\emph{-soft}, iff
        \[
            \fa{a \in A(s), s \in S:} \pi(a|s) \geq \frac{\epsilon}{|A(s)|}.
        \]
    \end{enumerate}
\end{definition}

Formulated slightly differently, an \( \epsilon \)-greedy policy chooses an optimal action w.r.t. \( q \) with probability \( 1-\epsilon \) and a random action \note{The random action might give us the optimal action aswell in this formulation!} chosen uniformly from \( A(s) \) with prob. \( \epsilon \).

\begin{corollary}[Greedy policy improvement]
         Let \( \pi \in \Pi \) and choose \( \pi' \in \Pi \) greedy w.r.t \( Q^\pi \). Then 
        \[
            V^\pi \leq V^{\pi'}.
        \]
\end{corollary}

With this Corollary we now have a new strategy to develop better and better policies:
Starting with an arbitrary policy \( \pi_{0} \in \Pi \), construct a sequence \( \left\{ \pi_n \right\}_{n \in \mathbb{N}} \subseteq \Pi \) of policies by choosing \( \pi_n \) greedy w.r.t \( Q^{\pi_{n-1}} \). But does this sequence converge (in the sense that the state value functions dont change anymore), and if it converges, will it give us an optimal policy, or just some sort of local maximum?  


\begin{theorem}[Optimailty via Policy Improvement]\label{optimality_via_improv}
    Let \( \pi \in \Pi \) and let \( \pi' \in \Pi \) be greedy w.r.t. \( Q^\pi \)\note{So we know, that \( \pi' \) will be atleast as good as pi already.}.
    If 
    \[
        V^\pi = V^{\pi'} \text{ or } Q^\pi = Q^{\pi'} \note{These conditions are actually equivalent and they mean that we are stuck in some local maximum.}
    \]
    then \( \pi \) and \( \pi' \) are already optimal.
\end{theorem}
\begin{proof}
By definition of \( Q^\pi \) and \( Q^{\pi'} \), it follows from \( V^\pi = V^{\pi'} \)that also \( Q^{\pi} = Q^{\pi'} \). The other direction follows from 
\[
     V^\pi(s) = \sum_{a \in A} Q^\pi (s|a) \cdot \pi(a|s) \note{This also is a consequence of the definition of \( Q \).}
\]
Hence we can assume both \(V^{\pi} = V^{\pi'} \text{ and } Q^\pi = Q^{\pi'} \). We start by writing
\begin{align*}\label{optimality_via_improv_eq}
    Q^{\pi'}(s|a) &= r(s,a) + \gamma \cdot \sum_{s' \in S} p(s'| s,a) V^{\pi'}(s') \\
                &= r(s,a) + \gamma \cdot \sum_{s' \in S, a' \in S} P(s'|s,a)\pi'(a'|s')Q^{\pi'}(s'|a') \\
                &= r(s,a) + \gamma \cdot \sum_{s' \in S, a' \in A} P(s'|s,a)\pi'(a'|s')Q^{\pi}(s'|a') \note{By the assumption for \( Q \).}\\
                &= r(s,a) + \gamma \cdot \sum_{s' \in S} P(s'|s,a)\max_{a' \in A(s')}Q^{\pi}(s'|a') \note{Since \( \pi' \) is greedy w.r.t \( Q^\pi \).}.\tag{\textasteriskcentered}
\end{align*}

Now we use
\begin{align*}
    V^{\pi'}(s') &= \sum_{a' \in A(s')} \pi'(a'|s')Q^{\pi'}(s'|a') \\
               &\leq \max_{a' \in A(s')} Q^{\pi'}(s'|a') \label{optimality_via_improv_eq2}\tag{\textasteriskcentered\textasteriskcentered}
\end{align*}
to obtain
\begin{align*}
    V^{\pi'}(s) &= \sum_{a \in A}\pi'(a|s) Q^{\pi'}(s|a) \\
              \overset{\eqref{optimality_via_improv_eq}}&{=} \sum_{a \in A} \pi'(a|s) \left[ r(s,a) + \gamma \cdot \sum_{s' \in S} P(s'|s,a)\max_{a' \in A(s')}Q^{\pi}(s'|a') \right] \\
              \overset{\eqref{optimality_via_improv_eq2}}&{\geq} \sum_{a \in A, s' \in S}\left[ r(s,a) + \gamma V^{\pi'}(s')  \right] \pi'(a|s)p(s'|s,a)\\
              &= \note{By the standard bellman equation} V^{\pi'}(s).
\end{align*}
But then we must have equality in the middle, and from this we are able to even obtain equality for each summand (since the summands already were \( \geq \)) and thus we also get equality in \eqref{optimality_via_improv_eq2}, i.e.
\begin{align*}
        V^{\pi'}(s') &= \max_{a' \in A(s')} Q^{\pi'}(s'|a')\\
                   &= \max_{a' \in A(s')} \left[ r(s',a') + \gamma \sum_{\widetilde{s} \in S} p(\widetilde{s} | s', a') V^{\pi'}(\widetilde{s}) \right].
\end{align*}
But this is the bellman \emph{optimality equation} for \( W := V^{\pi'}\), so we have 
\[
    (V^{\pi} = ) \,V^{\pi'} = V .
\]
Thus \( \pi, \pi' \) are optimal. 
\end{proof}


This means we don't end up in local maximums in case we we have the mentioned convergence. But do we have convergence?\note{Again, in the sense that the Value function stops changing (improving).} First we restate the last theorem:

\begin{corollary}[Suboptimality via Policy Improvement]
    Let \( \pi \in \Pi \), and \( \pi' \in \Pi \) greedy w.r.t \( Q^{\pi} \). If \( \pi \) is strictly suboptimal, there exists some \( s \in S \) such that 
    \[
        V^{\pi}(s) < V^{\pi'}(s).
    \]
\end{corollary}


Now suppose that we start with an arbitrary policy \( \pi_0 \in \Pi \) and construct an entire sequence \( \left\{ \pi_n \right\}_{n \in \mathbb{N}} \subseteq \Pi \) of policies by choosing \( \pi_n \) greedy w.r.t \( Q^{\pi_{n-1}} \). Observe that, by construction each \( \pi_n \) with \( n \geq 1 \) is deterministic \note{Simply by being \( 0 \) -greedy}. Moreover note, that there are only finitely many (\( |D| \)-many) determinisitc policies, i.e. the set \( \left\{ V^{\pi_n} \,|\, n \geq 1 \right\} \) is finite. As the \( V^{\pi_n} \) are pointwise increasing then only finitely many improvements of \( V^{\pi_n} \) can occur in the sequence, i.e. the sequence will stop changing after some \( n \geq N \) (at latest after \( N:=|D| \)), thus we have our desired convergence, i.e.
\[
    V^{\pi_n} = V^{\pi_{n+1}} \text{ for all } n \geq |D|. \note{Note that this doesn't neccessarily imply that \( \pi_n = \pi_{n+1} \text{ for } n \geq |D| \) since there might be multiple deterministic policies that are optimal.}
\]


There is only one problem with our procedure here:
It assumes that we are capable of computing state/action value functions \( V^\pi \) or \( Q^\pi \), associated with an arbitrary policy. This could e.g. be done by solving the linear system of equations arising from Bellman equation(s). But for this one needs to know the transition probability function \( p \) and the rewards \( r(s,a) \) ...


The case of unknown \( p \), which is at the heart of RL, is considered in the next chapter. This is also when \( \epsilon \)-greedy policies become increasingly important, as they are capable of \quotes{exploring} all admissible state-action pairs with small probabilities which greedy policies, which only \quotes{exploit} optimal actions are not capable of.

Before digging deeper into this matter we prepare via taking a closer look at how \( \epsilon \)-greedy policies behave in terms of policy improvement.


\begin{theorem}[\( \epsilon \)-greedy policy improvement]
    Let \( \epsilon \in (0,1] \). Suppose that \( \pi \in \Pi \) is \( \epsilon \)-soft (i.e. \( \pi(a|s) \geq \frac{\epsilon}{|A(s)|}) \) and \( \pi' \) is \( \epsilon \)-greedy w.r.t. \( Q^{\pi} \). Then 
    \[
        V^{\pi}(s) \leq V^{\pi'}(s) \text{ for all } s \in S.
    \]
\end{theorem}

\begin{proof}
Like for the \( 0 \)-greedy case we simply check the conditions of the policy improvement \cref{policy_improvement}: 
We have 
\begin{align*}
    V^{\pi}(s) &= \sum_{a \in A(s)} \pi(a|s) Q^{\pi}(s|a) \\
            &= \frac{\epsilon}{|A(s)|} \cdot \sum_{a \in A(s)} Q^{\pi}(s|a) + (1-\epsilon) \cdot \sum_{a \in A(s)} \frac{\pi(a|s) - \frac{\epsilon}{|A(s)|}}{1-\epsilon} Q^{\pi}(s|a) \note{Just adding a \( 0 \).}\\
            &\leq \frac{\epsilon}{|A(s)|}\cdot \sum_{a \in A(s)} Q^{\pi}(s|a) + (1-\epsilon) \cdot \max_{a \in A(s)} Q^{\pi}(s|a) \sum_{a \in A(s)} \frac{\pi(a|s) - \frac{\epsilon}{|A(s)|}}{1-\epsilon}
             \note{By the \( \epsilon \)-softness we have \( \pi(a|s) - \frac{\epsilon}{|A(s)} \geq 0 \), so the maximum indeed makes the term larger.}\\
            &= \frac{\epsilon}{|A(s)|}\cdot \sum_{a \in A(s)} Q^{\pi}(s|a) + (1-\epsilon) \cdot \max_{a \in A(s)} Q^{\pi}(s|a) \frac{1}{1-\epsilon} \cdot (1-\epsilon)\\
            &= \frac{\epsilon}{|A(s)|}\cdot \sum_{a \in A(s)} Q^{\pi}(s|a) + (1-\epsilon) \cdot \max_{a \in A(s)} Q^{\pi}(s|a) \\
            &= \sum_{a \in A(s)} \pi'(a|s) Q^{\pi}(s|a)
\end{align*}
which is exactly what we have to check, since 
\[
    V^\pi(s) = \sum_{a \in A(s)} \pi(a|s) Q^{\pi}(s|a).
\]
So we are finished.
\end{proof}
























\section{Temporal difference learning}
Now: towards reinforcement learning.\\
Setting: We do not know the transition probabilities
\[
s' \mapsto p(s'|s,a)
\]
but have an \quotes{oracle} which allows us to sample from \( p \) and any  
\( \pi \in \Pi \).
Reminder:
\begin{align*}
    V(s) &= \max_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot V(s') \right]\note{The Bellman optimality equation.} \\
    Q^{\pi}(s|a) &= r(s,a)
 + \gamma \cdot \sum_{s' \in S, a' \in A} p(s' | s,a) \cdot \pi(a' | s') \cdot Q^{\pi}(s'|a') \\
      V^{\pi}(s) &= \mathbb{E}_s^\pi \left[ \sum_{n=0}^\infty \gamma^n \cdot r(S_n, A_n) \right]\\
      &= \sum_{a \in A} \left[ r(s,a) + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot V^{\pi}(s') \right] \cdot \pi(a|s)
\end{align*} 

Observe that, in this setting,
\begin{itemize}
    \item we cannot solve the Bellman optimality equation for \( V \) as it explicitly involves \( p \).
    \item the policy improvement algorithm breaks down since we have no means of computing \( Q^{\pi} \) or \( V^{\pi} \) without knowing \( p \).     
\end{itemize}

\subsection{Temporal difference estimation}

We begin by investigating how we can estimate \( V^{\pi} \) and/or \( Q^\pi \) for a fixed policy \( \pi \in \Pi \) without knowing \( p \). The first, ad hoc, idea is to estimate 
\[
    V^{\pi}(s) = \mathbb{E}_s^\pi \left[ \sum_{n=0}^{\infty} \gamma^n \cdot r(S_n, A_n) \right]
\] 
using the law of large numbers.


To simplify notations, we write 
\[
    R_n := r(S_n, A_n), n \in \mathbb{N}_{0}
\] 
for the immedia reward at time \( n \in \mathbb{N}_{0} \).
Our assumption of having acess to an oracle means we can produce an arbitrary number \( M in \mathbb{N} \) of realizations 
\[
    r_n^1, \mydots, r_n^M \text{ of } R_n
\]
sampled independently under \( \mathbb{P}_s^\pi \).

To make this more formal, we will take a slightly different view point: instead of having different realizations (for different \( \omega_1, \mydots, \omega_M \) ) of the same process we wil have (at least) \( M \) different processes but only use one realization each at the same \( \omega \). Then we are able to use the standard definition of independence for random variables in our future assumptions \note{In the other view this independence would be hard to formalize.}.

So more precisely, lets assume that there are independet stochastic processes 
\[
    \left\{ \left( S_n^m, A_n^m \right) \right\}_{n \in \mathbb{N}_{0}}, m \in \mathbb{N}
\]
which have the same distribution \note{In short, they are i.i.d.}
as \( \left\{ \left( S_n, A_n \right) \right\}_{n \in \mathbb{N}_{0}} \) under \( \mathbb{P}_s^{\pi} \). Writing \( R_n^m := r(S_n^m, A_n^m) \), the oracle allows to obtain one fixed realization of the first \( M \in \mathbb{N} \) processes \( \left\{ \left( S_n,A_n, R_n \right) \right\}_{n \in \mathbb{N}_{0}}, m = 1, \mydots, M \) up to a fixed time step \( N \in \mathbb{N}_{0} \). That is, for some \( \hat{\omega} \in \Omega \) fixed (but unknown) we are given the values
\[
    s_n^m := S_n^m(\hat{\omega}), a_n^m := A_n^m(\hat{\omega}), r_n^m := R_n^m(\hat{\omega}), m = 1,\mydots, M,\, n = 0,1,\mydots,N
\]      

Now note that 
\[
    \sum_{n=0}^N \gamma^n R_n \simeq \sum_{n=0}^\infty \gamma^n R_n \text{ if N is large }.
\]
and by the strong lar of large numbers
\[
    \frac{1}{M}\sum_{m=1}^M \sum_{n=0}^N \gamma^n \cdot R_n^m \longrightarrow \mathbb{E}_s^\pi \left[ \sum_{n = 0}^N \gamma^n R_n \right] \mathbb{P}_s^\pi \text{-a.s.}
\]
In combination, it follows that
\begin{align*}
    \frac{1}{M} \sum_{ m = 1}^M \sum_{n=0}^N \gamma^n \cdot r_n^m &= \frac{1}{M} \sum_{m=1}^M \sum_{n = 0 }^N \gamma^n \cdot r(S_n^m(\hat{\omega}), A_n^m(\hat{\omega})) \\
    \overset{M \to \infty, \mathbb{P}_s^\pi\text{-a.s.}}&{\longrightarrow} \mathbb{E}_s^\pi \left[  \sum_{n = 0}^N \gamma^n \cdot r(S_n, A_n) \right] \\
    \overset{N \to \infty}&{\longrightarrow} \mathbb{E}_s^\pi \left[  \sum_{n=0}^{\infty} \gamma^n \cdot r(S_n, A_n) \right] = V^\pi(s)
\end{align*}
The quality of approximation clearly depends on the problem at hand. For example, the approximation in \( N \) depends on the runtime of a singe episode.
In general, there are three different situations which may arise.



\begin{definition}[absorbing \qquad terminal \qquad \( S_\dagger \) ]
A state \( s \in S \) is called \emph{absorbing}, iff 
\[
    p(s|s,a) = 1, \text{ for all  } a \in A(s).
\] 
Moreover \( s \) is called \emph{terminal}, iff it is absorbing and \( r(s,a) = 0 \) for all \( a \in A(s) \).
We write \( S_\dagger \) for the set of \emph{terminal states}.\note{We assume that we know the terminal states.}         

\end{definition}

Iff the state process \( \left\{ S_n \right\}_{n \in \mathbb{N}_{0}} \) enters into an absorbing state, it will never leave that state.
If the reward in that state is zero, the MDP has basically \quotes{terminated} -- the episode is effectively over.

\begin{definition}[lifetime \( \rho \)]
    The function \( \rho: \Omega \to \mathbb{N}_{0} \cup \left\{ +\infty \right\} \)
    \[
        w \mapsto \rho(\omega) := \inf \left\{ n \in \mathbb{N}_{0}: S_n(\omega) \in S_{\dagger} \right\}
    \] 
    is called \emph{terminal time / lifetime} of the MDM.
\end{definition}

There are exactly three different cases for the lifetime of a MDM:
\begin{itemize}
    \item finite horizon problems: There exists \( N \in \mathbb{N}_{0} \) s.t. for all \( s \in S, \pi \in \Pi \) we have 
    \( \rho \leq N \,\,\mathbb{P}_s^\pi\text{-a.s.}\).
    
    \item infinite horizon problems: For all \( s \in S, \pi \in \Pi \) we have  \( \rho = \infty \,\,\mathbb{P}_s^\pi\text{-a.s.} \).
    
    \item random horizon problems: Any problem which is not a finite or infinite horizon problem.
\end{itemize}

In what follows, we refer for each fixed \( m \in \left\{ 1,\mydots, M \right\} \) 
 to each sequence 
 \[
     s_{0}^m,a_{0}^m, r_{0}^m,\mydots,s_n^m,a_n^m, r_N^m
 \]

 produced by the oracle as a \emph{roll-out}. 

 Again, these are interpreted as realizations of the random variables
 \[
     S_{0}^m, A_{0}^m, r(S_{0}^m, A_{0}^m),\mydots, S_n^m, A_n^m, r(S_n^m, A_n^m).
 \]
In finite horizon problems, we can always guarantee, that the length \( N \) of a roll-out corresponds to the number of time steps in an episode, so there is no error in the approximation
\[
    \sum_{n=0}^N \gamma^n R_n^m \text{ of  } \sum_{n=0}^\infty \gamma^n R_n^m.
\]  
 
Conversely, in infinite horizon prolems, there will always be an error. In random time horizon problems one can try to choose \( N \) depending on \( m \) with the objective of observing an entire episode, albeit there is no guarantee that this is possible.
 
 

 
\subsection*{Update rules}
Before, our (random) approximation of \( V^\pi(s) \) after \( m \in \left\{  1,\mydots,M \right\}\) roll-outs all beginning in \( s \) takes the form 
\[
    \hat{V}_m^\pi(s) := \frac{1}{m} \sum_{l=1}^m \sum_{n=0}^N \gamma^n r(S_n^l, A_n^l)
\]
Note that \( \hat{V}_m^\pi \) can be computed iteratively w.r.t. \( m \) by 
\begin{align*}
        \hat{V}_{m+1}^\pi(s) &= \frac{1}{m+1} \sum_{l=1}^{m+1} \sum_{n=0}^N \gamma^n R_n^l \\
        &= \frac{1}{m+1} \sum_{n=0}^N \gamma^n R_n^{m+1} + \frac{m}{m+1} \cdot \frac{1}{m} \sum_{l=1}^m \sum_{n=0}^N \gamma^n R_n^l\\
        &= \frac{1}{m+1} \sum_{n=0}^N \gamma^n R_n^{m+1} + \frac{m}{m+1} \hat{V}_m^\pi(s)
\end{align*}
 
This update rule is much more memory efficient, as we do not have to store the rewards of the previous rollouts \( l = 1,..,m \). Here, for each rollout \( m \) we perform one update step, where all of the rollout is used once.
Rewriting the update rule above, by setting
\[
    \alpha_{m+1}(s) := \frac{1}{m+1}
\]
and using 
\[
    \frac{m}{m+1} = 1 - \frac{1}{m+1} = 1 - \alpha_{m+1}(s)
\] 
yields 
\[
    \hat{V}_{m+1}^\pi(s) = \hat{V}_m^\pi(s) + \alpha_{m+1}(s) \cdot \left[ \sum_{n=0}^N \gamma^n \cdot R_n^{m+1} - \hat{V}_m^\pi(s) \right]
\]
which is of the form
\[
    \text{ new estimate } = \text{ old est. } + \text{ learning rate } \cdot \text{ error in the (old) estimate }
\] 
 
Some tricks to make the estimation more efficient in practice:
\begin{itemize}
    \item Instead of sampling from \( \mathbb{P}_s^\pi \) one can sample from \( \mathbb{P}_\mu^\pi \), where \( \mu \) is a distribution on \( S \) which puts positive probability on each state \( s \in S \) \note{This doesnt give more efficiency on its own.}. The m-th roll out \( (s_n^m, a_n^m, r_n^m), n=0,\mydots,N \) is used to update the stimate of \( V^\pi(s_{0}^m) \) corresponding to the randomly chosen initial state.
    
    \item Each Rollout can be used to update \emph{several} different estimates of \( V^\pi \). Indeed, if for \( n \in \left\{ 1,\mydots,N \right\} \)
    \[
        s_n^m \notin \left\{ s_{0}^m, s_{1}^m, \mydots, s_{n-1}^m \right\}
    \]
    we can think of \( (s_j^m, a_j^m, r_j^m), j=n,\mydots,N \) as a roll out with initial state \( s_n^m  \). This approach is called \emph{first visit Moneta Carlo estimation}.
    The advantage is, that it requires less samples, compared to the original approach, in which each roll-out is only used once.
    The disadvantage is that the estimates for \( V^\pi(s_{0}^m) \) and \( V^\pi(s_n^m) \) become stochastically dependent, as they use realizations from the same random variables \( (S_j^m, A_j^m), j = n,\mydots,N \). However, the Law or Large numbers doesnt break down in this case yet.
    
    \item In practice, one often even goes one step further and uses \emph{every} subsequence of a roll out 
    \begin{align*}
        (s_{0}^m, a_{0}^m, \mydots, s_N^m, a_N^m) \text{ to update the estimate of } V^\pi(s_{0}^m) \\
        (s_{1}^m, a_{1}^m, \mydots, s_N^m, a_N^m) \text{ to update the estimate of } V^\pi(s_{1}^m) \\
        \vdots\qquad \qquad \qquad \qquad \qquad  \\
        (s_{N}^m, a_{N}^m) \text{ to update the estimate of } V^\pi(s_{N}^m) \\
    \end{align*}
\end{itemize}
 
Irrespective of whether or not a state has been visited before in the roll out, or not. This causes dependence between reward samples \emph{within} estimates of \( V^\pi(s_n^m) \) as soon as \( s_N^m \) is visited more than once in a roll out. This is called \emph{every visit Moneta-Carlo estimation}. Here (atleast the standard formulation of) the Law of large numbers can't be applied anymore, so it isn't clear at all, if this method will give convergence. It seems to work well in practice though.
 
\subsection*{Temporal difference estimation:}

In Monte Carlo we approximate 
\[
    V^\pi(s) = \mathbb{E}_s^\pi \left[ \sum_{n=0}^\infty \gamma^n \cdot r(S_n, A_n) \right].
\]

Instead, inspired by the Bellman equation, we could also try to approximate 

\[
    V^\pi(s) = \mathbb{E}_s^\pi \left[  r(S_{0}, A_{0}) + \gamma \cdot V^\pi(S_{1}) \right]
\]
This is achieved as follows. Instead of using the Monte carlo update rule 

\[
    \hat{V}_{m+1}^{\pi} (s) = \hat{V}_m^\pi(s) + \alpha_{m+1}(s) \cdot \left[ \sum_{n=0}^N \gamma^n \cdot r_n^{m+1} - \hat{V}_m^\pi(s) \right]
\]

we change the target as follows: Let \( l \in \left\{ 1,\mydots,M \right\}, n \in \left\{  0,\mydots,N-1 \right\} \) and write
\[
    (s,a,r,s',a') = (s_n^l, a_n^l, r(s_n^l, a_n^l), s_{n+1}^l, a_{n+1}^l  ).
\] 
Suppose moreover, we have an estimate \( \hat{V}_m^\pi(s) \) which has been updated \( m \)-times before. Then the update rule is
\[
    \hat{V}_{m+1}^\pi(s) = \hat{V}_m^\pi(s) + \alpha_{m+1}(s) \cdot \left[ r + \gamma \cdot \hat{V}_m^\pi(s') - \hat{V}_m^\pi(s) \right] \label{bellman_eq_inspired_estimation}\tag{+}\note{This will be called \( \operatorname{TD}(0). \) }
\] 
where \( \left\{ \alpha_m(s) \right\}_{m \in \mathbb{N}} \subseteq [0,\infty) \)  is a sequence, s.t. 
\[
    \sum_{m=1}^{\infty} \alpha_m(s) = \infty \text{  and } \sum_{m=1}^{\infty} \alpha_{m}(s)^{2} < \infty \text{ for each } s.\note{These are called the \quotes{Robbins-Monro} conditions.}
\] 

The update rule corresponds to an every visit Monte Carlo estimation in which the learning rate has been generalized (ever so slightly) and, more importantly, the target 
\[
    \sum_{n=0}^{\infty} \gamma^n R_n
\]
has been replaced with 
\[
    R_0 + \gamma V^\pi(S_1)
\]
inspired by the Bellman equation.

The estimation precedure based on \eqref{bellman_eq_inspired_estimation} is referred to as \emph{(zero step) temporal difference estimation}, or \( \operatorname{TD}(0) \) for short. We note here that there is also an \emph{n-step variant} \( \operatorname{TD}(n) \) in which the target is 
\[
    \sum_{k=0}^n \gamma^k R_k + \gamma^{n+1}V^\pi(S_{n+1}),
\]  
which we will not discuss any further. We merely note that every visit Monte Carlo estimation can be thought of as \( \operatorname{TD}(\infty) \).

Finally, let us note that there is also a version of temporal difference learning for the action value function. Since we may interpret 
\[
    Q^\pi(s|a) = \sum_{s' \in S, a' \in A} \left[ r(s,a) + \gamma Q^\pi(s' | a') \right] \cdot p(s'|s,a) \cdot \pi(a'|s')
\]

as 
\[
    Q^\pi(s|a) = \mathbb{E}_s^\pi \left[ r(S_{0},A_{0}) + \gamma \cdot Q^\pi(S_{1} | A_{1}) \,|\, A_{0} = a \right] \note{We dont prove this here tho.},
\]

we obtain an update rule for the estimate \( \hat{Q}_m^\pi(s|a)  \) of \( Q^\pi(s|a) \)  of the form
\[
    \hat{Q}^\pi_{m+1}(s|a) = \hat{Q}_m^\pi(s|a) + \alpha_{m+1}(s) \left[ r + \gamma \hat{Q}_m^\pi(s'|a') - \hat{Q}_m^\pi(s|a) \right].
\]
This estimation procedude is referred to as the \( \operatorname{SARSA}(0) \) estimation rule, as updates are produced from a sequence \( (s,a,r,s',a') \) of states, actions and a reward. Of course, there are also \( n \)-step variants \( \operatorname{SARSA}(n) \).






\subsection{Temporal Difference Control}
Now that we have found a way to estimate \( V^\pi \) and \( Q^\pi \) without explicitly knowing the transition probability function \( p \), let us devise a way of constructing good or even optimal policies without knowing \( p \).

The central idea is to combine the temporal difference estimation, or more precisely \( \operatorname{SARSA}(0) \), and policy improvement. The policy improvement step is performed after every update of our estimates for the action value function. This results in the following algorithm:

\begin{enumerate}
    \item Initialize a function \( q: S \times A \to \mathbb{R} \),
    \[
        (s,a) \mapsto q(s,a)
    \] arbitrarily (e.g. \( q \equiv 0 \)) except that \( q(s|a) = 0 \) if \( s \in S_\dagger, a \in A\).
    \item Initialize a learning rate function \( \alpha: S \times A \to [0, \infty] \) (e.g. \( \alpha \equiv 1 \) ).
    
    \item loop \( m=1,\mydots,M \) (looping over episodes)
    \begin{itemize}
        \item Initialize \( S_0^m \) (e.g. randomly)
        \item Choose an action \( A_0^m \) for \( S_0^m \) using a policy derived from \( q \) (typically \( \epsilon \)-greedy).
        \item loop over \( n=0,1,\mydots,N \) (looping over time steps)
        \begin{itemize}
            \item take action \( A_n^m \) and observe both the reward \( r(S_n^m, A_n^m) \) and the next state \( S_{n+1}^m \).
            \item choose an action \( A_{n+1}^m \) for \( S_{n+1}^m \) using a policy \note{policy improvement} derived from \( q \) (typically \( \epsilon \)-greedy ).
            \item update the function \( q \) by setting \note{\( \operatorname{SARSA}(0) \) estimation }
            \[
                \qquad \qquad \qquad q(S_n^m | A_n^m) \leftarrow q(S_n^m | A_n^m) + \alpha(S_n^m, A_n^m) \cdot \bigg[ r(S_n^m, A_n^m) + \gamma q(S_{n+1}^m | A_{n+1}^m)  - q(S_n^m | A_n^m)\bigg]
            \]
            \item update the learning rate function \( \alpha \) in the state \( S_n^m, A_n^m \) (e.g. \( \alpha(S_n^m , A_n^m) = \frac{1}{1+\#\text{visits of } (S_n^m, A_n^m)} \)  ).
            \item stop this inner loop early if \( S_n^m \) is a terminal state.      
        \end{itemize}    
    \end{itemize} 
\end{enumerate}


In plain words, the algorithm works by alternating between estimation of action value functions using \( \operatorname{SARSA}(0) \) and an update of the policy. For updating the policy, one might be inclined to use \textit{greedy} actions, instead of \( \epsilon \)-greedy actions, as this has given us the best chances of convergence to an \textit{optimal} policy in the past.\note{But note that we here only have acess to greedy actions w.r.t. a (possibly bad) \textit{approximation} of \( Q^\pi \)!} But this is not a good idea. Be aware that we have to balance optimization and estimation. The policy determines which states and actions are being updated in the estimation step. By choosing greedy actions, some state-action pairs might never be visited (altough these might optimal ones). This problem is referred to as the 
\begin{indented}
    \emph{exploration-exploitation}
\end{indented}
trade-off. In practice, one deals with this problem by choosing \( \epsilon \)-greedy actions instead. This ensures, that all state-action pairs are eventually visited. The downside of this approach is that, if \( \epsilon \) is kept fixed, we can at most attain an \( \epsilon \)-soft-optimal policy.


There is another way of dealig with this issue. Roughly, the idea is to use two different policies.

\begin{itemize}
    \item a \emph{behavioral} policy \( \pi_b \) which is used to generate the MDP. It should be chosen s.t. the state and action spaces are beeing explored (\( \epsilon \)-greedy, say).
    \item a  \emph{control} policy \( \pi^* \) which is used to update the estimator \( q \). Choosing this policy to be greedy w.r.t. \( q \) essentially means that we have to replace the \( \operatorname{SARSA}(0) \) target
    \[
        r(S_n^m, A_n^m) + \gamma \cdot q(S_{n+1}^m | A_{n+1}^m )
    \] 
    by 
    \[
         r(S_n^m, A_n^m) + \gamma \cdot \max_{a \in A(S_{n+1}^m)}q(S_{n+1}^m | a ).
    \]
\end{itemize}
This method is referred to as \emph{off-policy temporal difference control}, or more compactly and easier to remember \emph{Q-learning}.

The algorithm before (with the actual \( \operatorname{SARSA}(0) \) update) is called accordingly \emph{on-policy temporal difference control} or \emph{SARSA} for short.


Finally, there is another version of the algorithm called \emph{expected SARSA} where the target is given by 
\[
    r(S_n^m, A_n^m) + \gamma \cdot \sum_{a' \in A(S_{n+1}^m)} q(S_{n+1}^m | a') \pi_b(a' | S_{n+1}^m),
\]

where \( \pi_b \) is the behavioral policy. Finally, there are of course 
\( n \)-step variants of these algorithms.  













\subsection{Convergence of Q-Learning}
Aim: Establish conditions under which Q-learning converges.
Recall the Q-learning algorithm:

\begin{itemize}
    \item \emph{Initialization}: Choose an arbitrary function \( q_{0} : S \times A \to \mathbb{R} \) s.t. 
    \[
        q_{0}(s|a) = 0 \text{ for } s \in S_\dagger, a \in A.
    \]  
    Let moreover \( \left\{ \pi_t \right\}_{t \in \mathbb{N}_{0}} \) be a sequence of policies and \( \left\{ \hat{\alpha}_t \right\}_{t \in \mathbb{N}_{0}} \) be a sequence of learning rates valued in \( [0,\infty) \). Sample 
    \[
        S_{0} \text{ (e.g. randomly) }
    \]
    \[
        A_{0} \text{ according to } \pi_{0}(\cdot | S_{0})
    \]
    \item \emph{update}: For each \( t \in \mathbb{N}_{0} \) sample 
    \[
        S_{t+1} \text{ from } \begin{cases}
            &(S_t, A_t) \text{ if } S_t \text{ is \emph{not} a terminal state}\\
            &\text{abitrarily (e.g. randomly) otherwise. }\\
        \end{cases} 
    \] 
    \[
        A_{t+1} \text{ according to } \pi_{t+1}(\cdot | S_{t+1}).
    \]
    Define a learning rate function \( \alpha_{t+1}: S \times A \to [0,\infty) \) 
    \[
        \alpha_{t+1}(s,a) := \hat{\alpha}_{t+1} \cdot 1_{\left\{ (s,a) = (S_t, A_t) \right\}} \cdot 1_{\left\{ S_t \in S_\dagger \right\}} \note{We only update \( q_t \) to \( q_{t+1} \) at \( (S_t,A_t) \) and only if \( S_t \) isn't terminal. The terminality check can be left out - terminal states will always make \( q_{\cdot}(s_\dagger, \cdot) \) zero.    }
    \] 
    and let \( q_{t+1}: S \times A \to \mathbb{R} \) be given by 
    \[
        q_{t+1}(s,a) := q_{t}(s|a) + \alpha_{t+1}(s,a) \cdot \left[ r(s,a) + \gamma \max_{a' \in A(S_{t+1})}q_t(S_{t+1}| a') - q_{t}(s|a) \right]
    \] 
    for all \( (s,a) \in S \times A \). 
\end{itemize}


The main difference in the formulation used here is that we combine rollouts \( \left( S_n^m, A_n^m \right) \) into one large sequence \( \left\{ \left( S_t, A_t \right) \right\}_{t \in \mathbb{N}_{0}} \) and that we explicitly treat the updating procedure as producing a sequence of functions \( \left\{ q_t \right\}_{t \in \mathbb{N}_{0}} \). The policies \( \pi_t \) are kept general here, but could e.g. be \( \epsilon_{t} \)-greedy w.r.t. \( q_{t} \) for a sequence \( \left\{ \epsilon_{t} \right\}_{t \in \mathbb{N}_{0}} \subseteq [0,1] \).

\emph{Aim}: Find conditions under which \( \left\{ q_t \right\}_{t \in \mathbb{N}_{0}} \) converges to the optimal action value function \( Q \).

Our starting point is the following fixed-point characterization of \( Q \).

\begin{corollary}[Fixed-point characterization of \( Q \)]\label{fixed_point_characterization_Q}
Suppose \( q^* : S \times A \to \mathbb{R} \) is a fixed point of the operator \( \mathcal{T}^{*} \) defined by acting on functions \( q: S \times A \to \mathbb{R} \) as follows:
\[
    \mathcal{T}^*[q](s,a) := r(s,a) + \gamma \cdot \sum_{s' \in S} p(s'|s, a) \cdot \max_{a' \in A(s')} q(s' | a').
\]
Define
\[
     V*: S \to \mathbb{R},\, s \mapsto v*(s) := \sum_{a \in A(s)} \pi^* (a|s) \cdot q(s|a),
\]
where \( \pi^* \) is any greedy policy w.r.t. \( q^* \). Then 
\[
    q* = Q \text{  and  } v* = V \text{ and } \pi^* \text{ is optimal. }
\]
\end{corollary}


\begin{proof}
This was established in the proof of \cref{optimality_via_improv}.    
\end{proof}

In light of \cref{fixed_point_characterization_Q} it becomes apparent what we have to do:
Show that \( \left\{ q_t \right\}_{t \in \mathbb{N}_{0}} \) converges to a fixed point of \( \mathcal{T}^* \).  

To see that there actually is a chance that Q-learning converges let us rewrite the update rule one last time as 
\[
    q_{t+1} = q_{t} + \alpha_{t} \cdot \left[ \mathcal{T}^*[q_{t}] - q_{t} + \epsilon_{t} \right], t \in \mathbb{N}_{0}
\]
where 
\[
    \epsilon_{t}(s,a) := r(s,a) + \gamma \max_{a' \in A(S_{t+1})} q_{t}(S_{t+1}| a') - \mathcal{T}^{*}[q_{t}](s,a)
\]
for all \( (s,a) \in S \times A \).

\begin{remark}
\( \left\{ q_{t} \right\}_{t \in \mathbb{N}_{0}} \) is a random sequence, since in each step \( t+1 \) the state-action pair which is updated is chosen randomly as \( (S_{t},A_{t}) \).       
\end{remark}

\begin{lemma}[Ingredients for convergence]
    \ \vspace{-2em}
    \begin{enumerate}[(i)]
        \item Suppose \( \gamma < 1 \). Then \( \mathcal{T}^* \) is a contraction w.r.t. \( ||\cdot||_{\infty} \)

        \item \( \displaystyle\sum_{s' \in S} \alpha_{t+1}(s,a) \cdot \epsilon_t(s,a) \cdot p(s' | S_t, A_t) = 0 \) for all \( (s,a) \in S \times A \).  
    \end{enumerate}
\end{lemma}

\begin{proof}
    \begin{enumerate}[(i)]
        \item Let \( q, \hat{q}: S \times A \to \mathbb{R} \) and \( (s,a) \in S \times A \). Suppose w.l.o.g. \( \mathcal{T}^* [q] (s,a) \geq \mathcal{T}^*[\hat{q}](s,a) \). Then 
        \begin{align*}
            & |\mathcal{T}^* [q](s,a) - \mathcal{T}^*[\hat{q}](s,a) | \\
            &=  \mathcal{T}^* [q](s,a) - \mathcal{T}^*[\hat{q}](s,a) \\
            &= r(s,a) + \gamma \sum_{s' \in S}p(s' | s,a) \cdot \max_{a* \in A(s')} q(s'|a') - r(s,a) - \gamma \sum_{s' \in S} p(s'|s,a) \cdot \max_{a' \in A(s')} \hat{q}(s'|a')\\
            &= \gamma \sum_{s' \in S} p(s' | s,a) \cdot \left[ \max_{a' \in A(s)} q(s'|a') - \max_{a' \in A(s')}\hat{q}(s'|a')  \right]\\
            &\leq \gamma \cdot \max_{(s',a') \in S \times A} | q(s'|a') - \hat{q}(s'|a')| \cdot \sum_{s' \in S} p(s' | s,a)\\
            &\leq \gamma \cdot ||q-\hat{q}||_{\infty} \cdot 1,
        \end{align*}   

    so 
    \[
        || \mathcal{T}^* [q] - \mathcal{T}^*[\hat{q}]||_{\infty} \leq \gamma \cdot ||q - \hat{q}||_{\infty}.
    \]

    \item Let \( (s,a) \in S \times A \) and recall 
    \[
        \epsilon_t(s,a) = r(s,a) - \gamma \max_{a' \in A(S_{t+1})} q_t(S_{t+1} | a') - \mathcal{T}^* [q_t](s,a)
    \] 
    and 
    \[
        \mathcal{T}^*[q_t](s,a) = r(s,a) - \gamma \cdot \sum_{s' \in S} p(s'| s,a) \max_{a' \in A(s)} q_t(s'|a').
    \]
    This implies 
    \begin{align*}
        &\alpha_{t+1}(s,a) \epsilon_t(s,a) \\ 
        &= \gamma \cdot \hat{\alpha}_{t+1} \cdot 1_{\left\{  (s,a) = (S_t, A_t) \right\}} \cdot 1_{\left\{  S_t \in S_{\dagger} \right\}}\\
        &\quad \quad \quad \cdot \left[ \max_{a' \in A(S_{t+1})} q_t(S_{t+1} | a') - \sum_{ s' \in S} p(s' | s,a) \max_{a' \in A(s')} q_t(s'|a') \right]\\
        &= \gamma \cdot \hat{\alpha}_{t+1} \cdot 1_{\left\{  (s,a) = (S_t, A_t) \right\}} \cdot 1_{\left\{  S_t \in S_{\dagger} \right\}}\\
        &\quad \quad \quad \cdot \left[ \max_{a' \in A(S_{t+1})} q_t(S_{t+1} | a') - \sum_{ s' \in S} p(s' | S_t,A_t) \max_{a' \in A(s')} q_t(s'|a') \right],
    \end{align*}
    so
    \[
        \sum_{s' \in S} \alpha_{t+1}(s,a) \epsilon_t(s,a) p(s' | S_t, A_t) = 0.
    \]
\end{enumerate}
    
\end{proof}


The previous lemma allows us to get an idea of why Q-learning has a chance to converge. 
Knowing that \( \mathcal{T}^* \) is a contraction, the classical Banach fixed point iteration reads 
\begin{align*}
    q_{t+1} &= \mathcal{T}^*[q_{t}] \\
            &= q_t + 1 \cdot \left[  \mathcal{T}^*[q_t] - q_t + 0 \right].\note{1 stands in for the learning rate \( \alpha_{t+1} \), 0 for the epsilon \( \epsilon_t \) . }
\end{align*}

Since we cannot evaluate \( \mathcal{T}^* \) -- as we do not know the transition probabilities \( p \) -- the Q-learning instead uses the iteration 
\[
    q_{t+1} = q_t + \alpha_{t+1} \cdot \left[ \mathcal{T}^*[q_t] - q_t + \epsilon_t \right].
\]  
For \( \alpha_{t+1} \equiv 1\), this would yield the fixed point iteration up to the additional error term \( \epsilon_{t} \).

While according to the previous lemma the error is unbiased (i.e. it vanishes in the mean), the Q-learning algorithm would not stand a chance to converge if \( \alpha_{t+1} \) is kept equal to \( 1 \).
Instead the role of \( \alpha_{t+1} \) is to balance the momevement towards the fixed point of \( \mathcal{T}^* \) (\( \alpha_{t+1} \) close to \( 1 \)  ) and fading out the contribution of the error term (\( \alpha_{t+1} \) close to \( 0 \)  ). This means that we would like 
\[
    \alpha_{t+1} \to 0, \text{ as } t \to \infty
\]
but not too fast, so that the update rule does not differ too much from the Banach fixed point iteration. The appropriate speed turns out to be given by the Robbins-Monro condition 
\[
    \sum_{t \in \mathbb{N}_{0}} \alpha_t (s,a) = \infty
\]
\[
     \sum_{t \in \mathbb{N}_{0}} \alpha_t (s,a)^2 < \infty
\]
for all \( (s,a) \in S \times A \).




\begin{theorem}[Convergence of Q-learning]
Suppose that the Q-learning algorithm is set up such that the sequence \( \left\{  \alpha_t \right\}_{t \in \mathbb{N}_{0}} \) satisfies the Robbins-Monro condition. Suppose moreover that \( \gamma < 1 \).

Then \( \left\{ q_t \right\}_{t \in \mathbb{N}_{0}} \) converges to the (unique) fixed point of \( \mathcal{T}^* \).
\end{theorem}

\begin{proof}Beyond the scope of this course. Uses super-martingale theory.
\end{proof}



Takign a look at the definition of \( \alpha_t \), we see that we need, in particular, that every state is visited infinitely often. This can be ensured by choosing the policies \( \left\{ \pi_t \right\}_{t \in \mathbb{N}_{0}}\) \( \epsilon \)- greedy and choosing the initial state for each episode randomly.



















\section{Policy Gradient Methods}


Reminder: \emph{Gradient Descent (GD) and Gradient ascent (GA)}
GD/GA is a class of methods to approximate minima of continously differentiable functions 
\[
    f : \mathbb{R}^d \to \mathbb{R}
\]
such that \( \argmin_{x \in \mathbb{R}^d} f(x) \neq \emptyset \) numerically. The simplest variant of GD starts at an arbitrary point \( x_{0} \in \mathbb{R}^d \), picks a learning rate \( \alpha > 0 \), and iteratively approximates minimizers of \( f \) via the update rule 
\[
    x_{t+1} := x_{t} - \alpha Df(x_t), t \in \mathbb{N}_{0},
\]   
where \( Df \) denotes the gradient of \( f \). The motivation for the update rule is that \emph{-\( Df(x_t) \)} is the direction of steepest descent of \( f \) at \( x_t \), hence \( f(x_{t+1}) \leq f(x_t) \) if \( \alpha \) is small enough. 

GD terminates, that is \( x_{t+1} = x_{t} \), iff \( Df(x_t) = 0 \), meaning that \( x_t \) is a stationary point of \( f \).\note{It could also very well be only a \emph{local} minimum, a saddle point or even a maximum!} For sufficiently nice \( f \) and small enough \( \alpha \), there are very simple and satisfactory convergence proofs.



\subsection{Policy Gradients and REINFORCE}

How can we apply GD to MDPs?

By replacing \( f \) by \( -f \), it is clear that GD can also be used to approximate global maxima, in which case we speak of gradient ascent (GA).
The function we wish to maximize is   
\[
    \pi \mapsto V^{\pi}(s)\note{One can replace the policy space by a \( \mathbb{R}^d \) where \( d := |S \times A|\) and apply GA to this problem. But thats a lot of dimensions!}
\]
for some fixed initial state \( s \in S \).  If we want to apply GA, we need to compute the gradient of the mapping above w.r.t. \( \pi \). In practice however, one typically first restricts the set of policies to a carefully chosen parameterized subset 
\[
     \Pi_\Theta := \left\{ \pi_\theta : \pi \in \Theta \right\} \subseteq \Pi .
\]
In doing so, one hopes to make the optimization more tractable/efficient by restricting the optimazion to a lower dimensional space \( \Theta \). On the other hand \( \Pi_\Theta \) should be large enough to contain an optimal polcy (at least in its \quotes{closure}). A common choice is 
\[
    \Theta := \mathbb{R}^d \text{ for some  } d \leq |S|\cdot|A|
\]  
and 
\[
    \pi_\theta(a|s) := \frac{\exp h_\theta(s,a)}{\sum_{a' \in A} \exp h_\theta(s,a')},\,\, a \in A(s)
\]
where \( \theta \mapsto h_\theta(s,a) \) is a continuously differentiable function for each \( (s,a) \in S \times A \).  


One such choice would be 
\[
    h_\theta(s,a) = \sum_{i=1}^d \theta_i \phi_i(s,a), \,\, \theta \in \mathbb{R}^d
\]
for functions \( \phi_{1},\mydots,\phi_d : S \times A \to \mathbb{R} \).

Another common choice for \( h_{0} \) is a neural network mapping from \( S \times A  \to \mathbb{R}\) with \( \theta \) denoting the parameters of the network. 
Having fixed \( \Pi_\Theta \), we can now think of 
\[
    \pi \mapsto V^\pi(s)
\]
restricted to \( \Pi_\Theta \) as a mapping 
\[
    \theta \mapsto V^{\pi_\theta}(s)
\] 
on \( \Theta \). In light of this, we can subsequently define for some fixed \( s \in S \) the mapping 
\[
    v: \Theta \to \mathbb{R}, \,\, \theta \mapsto v(\theta) := V^{\pi_\theta}(s)
\]
and assume \( \Theta \subseteq \mathbb{R}^d \) open\note{For differentiability reasons.}.To simplify the exposition, we furthermore assume from now on, that we are in a finite time horizon setting.\note{This lets us not worry about interchanging infinite series and differentiation.}


\emph{Assumption:} There exists \( N \in \mathbb{N}_{0} \) such that 
\[
    S_{N+1} \in S_\dagger, \quad \mathbb{P}_s^\pi\text{-a.s.} \text{ for all } \pi \in \Pi, s \in S.
\]
This assumption means that any episode runs for at most 
\( N + 1 \) time steps.

\begin{theorem}[Policy Gradients]
Suppose that each \( \pi_\theta, \theta \in \Theta\) is soft and
\[
    \theta \mapsto \pi_\theta(a|s), \,\, \theta \in \Theta
\]
is continuously differentiable for all \( (s,a) \in S \times A \). 
Then \( \theta \mapsto v(\theta) \) is continuously differentiable and
\[
    Dv(\theta) = \mathbb{E}_s^{\pi_\theta} \left[ \sum_{n = 0}^N \big( D \left[ \log(\pi_\theta(A_n | S_n)) \right] \cdot \sum_{m = n}^N \gamma^m r(S_m, A_m)\big) \right].
\]
\end{theorem}

\begin{proof}
\begin{enumerate}[(i)]
    \item First, note that 
    \begin{align*}
        &v(\theta) \\
        &= v^{\pi_\theta}(s) \\
        &= \mathbb{E}_s^{\pi_\theta} \left[ \sum_{n=0}^\infty \gamma^n r(S_n, A_n) \right]\\
        &= \mathbb{E}_s^{\pi_\theta} \left[ \sum_{ n=0}^N \gamma^n r(S_n, A_n) \right]\note{Since we assume finite time horizon.}\\
        &= \sum_{(s_{0},a_{0}),\mydots,(s_N, a_N) \in S \times A}\, \sum_{ n = 0}^N \gamma^n r(s_n, a_n) \mathbb{P}_s^{\pi_\theta} \left[ S_{0} = s_{0}, A_{0} = a_{0},\mydots, S_N = s_N, A_N = a_N \right]\\
        &= \sum_{(s_{0},a_{0}),\mydots,(s_N, a_N) \in S \times A}\, \sum_{ n = 0}^N \gamma^n r(s_n, a_n) \delta_s[{s_{0}}] \pi_\theta(a_{0}|s_{0}) \prod_{k=1}^{N} p(s_k | s_{k-1}, a_{k-1}) \pi_\theta(a_k, s_k)
    \end{align*}
    From this and our assumption, we see that \( \theta \mapsto v(\theta) \) is cont. differentiable. 

    \item We compute the gradient of \( \mathbb{P}_s^{\pi_\theta} \left[ S_{0} = s_{0}, A_{0} = a_{0},\mydots, S_N = s_N, A_N = a_N \right] \).
    Using that for any event \( B \in \mathcal{A} \) with \( \mathbb{P}_s^{\pi_\theta}[B] > 0 \)  we have
    \begin{align*}
         D\mathbb{P}_s^{\pi_\theta}[B] &= \frac{D\mathbb{P}_s^{\pi_\theta}[B]}{\mathbb{P}_s^{\pi_\theta}[B]} \cdot \mathbb{P}_s^{\pi_\theta}[B] \\
         &= D \log(\mathbb{P}_s^{\pi_\theta}[B]) \cdot \mathbb{P}_s^{\pi_\theta}[B],
    \end{align*}
    it follows that 
    \begin{align*}
        &\mathbb{P}_s^{\pi_\theta} \left[ S_{0} = s_{0}, A_{0} = a_{0},\mydots, S_N = s_N, A_N = a_N \right] \\
        &= D \left[ \log(\mathbb{P}_s^{\pi_\theta} \left[ S_{0} = s_{0}, A_{0} = a_{0},\mydots, S_N = s_N, A_N = a_N \right]) \right] \cdot \mathbb{P}_s^{\pi_\theta} \left[ S_{0} = s_{0}, A_{0} = a_{0},\mydots, S_N = s_N, A_N = a_N \right]\\
        &= D \left[ \log(\delta_s[\left\{ s_{0} \right\}]) +  \sum_{k=1}^N \log(p(s_k|s_{k-1, a_{k-1}})) + \sum_{k=0}^N \log(\pi_\theta(a_k|s_k)) \right]\\
        &\qquad \qquad \qquad \cdot \mathbb{P}_s^{\pi_\theta} \left[ S_{0} = s_{0}, A_{0} = a_{0},\mydots, S_N = s_N, A_N = a_N \right]\\
        &= \sum_{k=0}^N D[\log(\pi_\theta(a_k | s_k))] \cdot \mathbb{P}_s^{\pi_\theta} \left[ S_{0} = s_{0}, A_{0} = a_{0},\mydots, S_N = s_N, A_N = a_N \right].
    \end{align*}

    \item Using (ii) it follows that 

    \begin{align*}
        Dv(\theta) &= \sum_{(s_{0},a_{0}),\mydots,(s_N, a_N) \in S \times A}\, \sum_{ n = 0}^N \gamma^n r(s_n, a_n) D\mathbb{P}_s^{\pi_\theta} \left[ S_{0} = s_{0}, A_{0} = a_{0},\mydots, S_N = s_N, A_N = a_N \right]\\
        &= \sum_{(s_{0},a_{0}),\mydots,(s_N, a_N) \in S \times A}\, \sum_{ n = 0}^N \gamma^n r(s_n, a_n) \sum_{k=0}^N D \left[ \log(\pi_\theta(a_k|s_k)) \right]  \cdot \mathbb{P}_s^{\pi_\theta} \left[ S_{0} = s_{0}, A_{0} = a_{0},\mydots, S_N = s_N, A_N = a_N \right]\\
        &= \mathbb{E}_s^{\pi_\theta} \left[ \sum_{k=0}^N D [\log(\pi_\theta(A_k | S_k))] \sum_{n=0}^N  \gamma^n r(S_n, A_n) \right]\\
        &= \sum_{n,m = 0}^N \mathbb{E}_s^{\pi_\theta}  \bigg[  D[\log(\pi_\theta(A_n | S_n))] \gamma^m r(S_m, A_m) \bigg].
    \end{align*}

    \item It remains to show that
    \[
        \mathbb{E}_s^{\pi_\theta} \left[  D [\log(\pi_\theta(A_n | S_n))] r(S_m, A_m) \right] = 0 \text{ if } m < n.
    \]
    For this, note that
    \begin{align*}
        &\quad \mathbb{E}_s^{\pi_\theta} \left[  D [\log(\pi_\theta(A_n | S_n))] r(S_m, A_m) \right]\\
        &= \sum_{(s_{0},a_{0}),\mydots,(s_N, a_N) \in S \times A} D [\log(\pi_\theta(a_n,s_n))] r(s_m, a_m) \cdot \mathbb{P}_s^{\pi_\theta} \left[ S_{0} = s_{0}, A_{0} = a_{0},\mydots, S_N = s_N, A_N = a_N \right]\\
        &= \sum_{s_{0} \in S} \sum_{a_{0} \in A} \mydots \sum_{s_n \in S} \sum_{ a_n \in A} D [\log(\pi_\theta(a_n | s_n))] r(s_m, a_m)\\
        & \qquad \qquad \cdot \delta_s[\left\{ s_{0} \right\}] \pi_\theta(a_{n},s_{n}) \prod_{k=1}^n p(s_k | s_{k-1}, a_{k-1}) \pi_\theta(a_{k-1} | s_{k-1})\\
        &= \sum_{s_{0} \in S} \sum_{a_{0} \in A} \mydots \sum_{s_m \in S} \sum_{ a_m \in A} \delta_s[\left\{ s_{0} \right\}] \prod_{k=1}^m p(s_k | s_{k-1, a_{k-1}}) \pi_\theta(a_{k-1 | s_{k-1}}) \sum_{a_m \in A} \pi_\theta(a_m | s_m) r(s_m, a_m) \\
        & \qquad \qquad \cdot \sum_{s_{m+1} \in S} p(s_{m+1} | s_m, a_m) \sum_{a_{m+1} \in A} \pi_\theta(a_{m+1} | s_{m+1}) \\
        & \qquad \qquad \cdot \mydots\\
        & \qquad \qquad \cdot \sum_{s_{n} \in S} p(s_{n} | s_{n-1}, a_{n-1}) \sum_{a_{n} \in A} \pi_\theta(a_{n} | s_{n}) D \log(\pi_\theta(a_n | s_n))\\
        &= 0,
    \end{align*}
    since the last sum 
    \[
       \sum_{a_{n} \in A} \pi_\theta(a_{n} | s_{n}) D \log(\pi_\theta(a_n | s_n)) = \sum_{a_n \in A} D \pi_\theta(a_n | s_n) = D \sum_{a_n \in A} \pi_\theta(a_n |s_n) = D 1 = 0.
    \]
\end{enumerate}    
\end{proof}



Having computed the gradient of \( v(\theta) \), it follows that the updating rule for GA with learning rate \( \alpha > 0 \) becomes
\begin{align*}
    \theta_{t+1} &= \theta_t + \alpha \cdot Dv(\theta) \\
            &= \theta_t + \alpha \cdot \mathbb{E}_s^{\pi_\theta} \left[  \sum_{ n=0}^N D[\log(\pi_\theta(A_n | S_n))] \sum_{m=n}^N \gamma^m r(S_m, A_m) \right], \,\, t \in \mathbb{N}_{0}
\end{align*}  
for some initial parameter \( \theta_{0} \in \Theta \).

In practice however, we typically do not have a means to compute the actual expectation in the update rule. Hence we estimate the expectation using Monte Carlo, leading to an update rule of the form

\begin{equation}\label{REINFORCE update rule}\tag{\textasteriskcentered}
     \theta_{t+1} = \theta_t + \alpha \cdot \sum_{ n = 0}^N D \log(\pi_{\theta_t}(A_n^{t+1} | s_n^{t+1})) \sum_{m=n}^N \gamma^m r(S_m^{t+1}, A_m^{t+1}), \,\, t \in \mathbb{N}_{0}.
\end{equation}
In general, using Monte Carlo estimates in GA as above is referred to as
\emph{stochastic gradient ascent (SGA)}. Hence, we move in the direction of steepest ascent only in the mean.

The application of SGA to MDPs in \eqref{REINFORCE update rule} is referred to as the \emph{REINFORCE} algorithm.
Being based on GA, we note that there is generally no guarantee that REINFORCE converges to a global maximum, but can be shown to converge to a stationary point of \( v(\theta) \) under appropriate conditions. 













\end{document}