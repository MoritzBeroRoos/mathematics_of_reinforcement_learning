% !TeX root = main.tex
\input{template/layout.tex}
\input{template/math.tex}
\input{course_specific.tex}





\title{Mathematics of Reinforcement Learning}

\author{Notes by Moritz Roos}
\date{}
%remove page number from tableofcontents page
%\AtBeginDocument{\addtocontents{toc}{\protect\thispagestyle{empty}}}
\begin{document}
\maketitle

\tableofcontents


\clearpage
\section{Introduction}
\textbf{Goals:}
\begin{itemize}
    \item Look at different types of problems.
    \item Learn the basic principles of Reinforcement Learning (RL).
    \item Lean when to apply RL and when not to?
\end{itemize}

\subsection{It's-a me, Mario!}
We want to fix some terminology by the example of the Super Mario game.
\begin{itemize}
    \item \emph{Agent:} player
    \item \emph{Environment:} game
    \item \emph{state:} the frame/screen being presented to the agent
    \item \emph{Action:} input on the controller
    \item \emph{policy:} a correspondence between states and actions
    \item \emph{episode:} one run of the game from start to finish
    \item \emph{reward:} feedback mechanism telling you how good/bad an action performs in a given state
    \item \emph{return:} criterion to be optimized, typically cumulative (discounted, expected) rewards
    \item \emph{state dynamics:} how the next state is obtained from the current state and the current action
\end{itemize}


\begin{figure}[ht]
    \centering
    %\caption{}
    %\incsvg{path/}{path/file}
    \incsvg{figures}{figures/examplemodell}
    \label{fig:examplemodell}
\end{figure}




\section{Markov decision processes}

\textbf{Goals:}
\begin{itemize}
    \item Develop a mathematical framework for dynamic decision making problems under uncertainty.
    \item Lean how to solve these problems if the model is known.
\end{itemize}

\subsection{Definition of markov decision processes}
We look for stochastic processes (a family of random variables indexed by time) \( \{S_n\}_{n \in  \mathbb{N}_0 }, \{A_n\}_{n \in \mathbb{N}_{0}} \{R_n\}_{n \in \mathbb{N}_{0}} \) modelling the dynamic evolution of states, actions and rewards.

\begin{definition}[Markov decision model]
    A \emph{Markov Decision Model} is a tuple \( (S, A, D, p, r ,\gamma) \) consisting of the following components:
    \begin{enumerate}
        \item a finite set \( S \)  called \emph{state space}.
        \item a finite set \( A \) called \emph{action space}
        \item a set \( D \subseteq S \times A \) whose elements are the \emph{admissible state-action pairs}
        \item a \emph{transition probability function} \( p: S \times S \times A \implies [0,1] \)
              \[
                  (s', s, a) \mapsto p(s' | s, a)\note{This is the probability of ending up in \( s' \) when performing action \( a \) in state \( s \). \( s' \mapsto p(s'| s,a) \) is a probability mass function.}
              \] satisfying
              \[
                  \sum_{s' \in S}p(s' | s, a) = 1
              \] for all \( (s,a) \in S \times A \).
        \item a \emph{reward function} \( r: D \to \mathbb{R} \)
              \[
                  (s,a) \mapsto r(s,a)\note{The reward for performing action \( a \)  in state \( s \).}
              \]
        \item a \emph{discount factor} \( \gamma \in (0,1] \).\note{This encodes the time-value of rewards. \( \gamma^{-1}\cdot r(s,a) \) is the value at time \( 0 \) of receiving \( r(s,a) \) \( n \) steps into the future.}

    \end{enumerate}
\end{definition}


\begin{definition}[Policy \qquad \( \Pi \)  \qquad \( \Pi_d \) ]\label{def_policy}
    A \emph{policy} is a mapping \( \pi: S \times A \to [0,1] \)
    \[
        (s,a) \mapsto \pi(a|s)
    \]
    such that
    \[
        \sum_{a \in A} \pi(a|s) = 1 \note{This means that \( a \mapsto \pi(a|s) \) is a probability mass function for each fixed \( s \) . Thus one action \textit{has} to be chosen.}
    \] for all \( s \in S \) and
    \[
        \pi(a|s) = 0
    \] for all \( s,a \in S\times A \setminus D\).

    We say that \( \pi \) is a \emph{deterministic policy} if
    \[
        \fa{s \in S:}\ex{a \in A:}\pi(a|s) = 1.
    \] We write \( \Pi \) and \( \Pi_d \) for the set of all policies and the subset of deterministic policies respectively.

\end{definition}

To wit, \( \pi(a|s) \) is interpreted as the probability of choosing action \( a \) in state \( s \). One could also introduce \quotes{non-Markovian} policies which depend on the entire history of states and actions or \quotes{non-stationary} policies which depend on the current state and current time.
However, we spare ourselves the trouble, since we will eventually see that there always is a \quotes{stationary, Markovian} optimal policy as in \cref{def_policy}.


\ \\
Intuitively, what happens for a given MDM and a policy \( \Pi \) is the following:

\begin{itemize}
    \item Start in an initial state \( S_{0}:= s_{0} \).
    \item Using the policy \( \pi \), randomly draw an action \( A_{0} \) from \( a \mapsto \pi(a|S_{0}) \).
    \item Collect the reward \( R_{0} := r(S_{0}, A_{0})\) and draw the next state \( S_{1} \) from \( s' \mapsto p(s'|S_{0}, A_{0}) \).
    \item repeat this procedure to construct \( S_{0}, A_{0}, R_{0}, S_{1}, A_{1}, R_{1},\mydots \)
\end{itemize}

Informally, we can then introduce the objective
\[
    \mathbb{E} \left[ \sum_{n=0}^{\infty} \gamma^{n} \cdot R_n \right] = \mathbb{E} \left[ \sum_{n=0}^{\infty} \gamma^{n} \cdot r(S_{n}, A_{n}) \right].
\]


Now we want to construct the mentioned stochastic processes. They should be consistent with our definition of MDM's and policies.\note{This can be seen as the formalization of the intuitive description of what should happen for a given MDM and a policy.} This enables us to make sense of notions like the expected return mentioned above.

For this, the first ingredient is a probability space. We choose the sample space as 
\[
    \Omega := (S \times A)^\infty = \bigtimes_{n=0}^{\infty} (S \times A)
\]
and let the \( \sigma \)-algebra \( \mathcal{A} \) on \( \Omega \) be the power set \( \mathcal{P}(\Omega) \) of \( \Omega \). We can safely do so, since \( \Omega \) is countable. In fact, any element \( w \in \Omega  \) takes the form 
\[
    \omega = (s_{0},a_{0},s_{1},a_{1},s_{2},a_{2},\mydots)
\]  
where \(  \left\{ s_n \right\}_{n \in \mathbb{N}_{0}} \subseteq S, \left\{ a_n \right\}_{n \in \mathbb{N}_{0}} \subseteq A \). With that in mind, we define functions
\[
    S_n(\omega) = S_n((s_{0},a_{0},s_{1},a_{1},\mydots)) := s_n
\]
\[
    A_n(\omega) = A_n((s_{0},a_{0},s_{1},a_{1},\mydots)) := a_n  
\]
for \( n \in \mathbb{N}_{0}, \omega \in \Omega \). 

Then \( S_n, A_n \) trivially\note{Trivially, since we chose the power set as \( \sigma \)-algebra. } are measurable functions ( = random variables) taking values in \( S, A \) respectively.  What is left is showing that there exists a probability measure \( \mathbb{P}^\pi_\mu \) on \( (\Omega, \mathcal{A}) \) such that 
\( S_{0},A_{0}, S_{1},A_{1},\mydots \) have the desired distribution.\note{The \( A \) 's should be determined by the policy and the last state. The \( S \) 's by the transition function and the taken action.}
This can be constructed from \( p, \pi \)  and an initial distribution \( \mu \) on \( S \) such that \( S_{0} \sim \mu \).

What we would like to do is define
\begin{align*}
    \mathbb{P}_\mu^\pi [\left\{ \omega \right\}] &= \mathbb{P}_\mu^\pi[{(s_{0},a_{0},s_{1},a_{1},s_{2},a_{2},\mydots)}] \\
    &:= \mu[\left\{ s_{0} \right\}] \cdot \pi(a_{0} | s_{0}) \cdot p(s_{1} | s_{0},a_{0}) \cdot \pi(a_{1} | s_{1}) \cdot p(s_{2}|s_{1},a_{1}) \cdot \pi(a_{2}|s_{2}) \cdot \mydots \\
    &= \mu[\left\{ s_{0} \right\}] \pi(a_{0},s_{0}) \cdot \prod_{n=1}^\infty p(s_n | s_{n-1}, a_{n-1}) \pi(a_n | s_n) \\
\end{align*}
for all \( \omega = (s_{0},a_{0},s_{1},a_{1},\mydots) \in \Omega \).
The problem is, however, that the resulting probability is usually zero, as this is an infinite product with factors valued in \( \left[ 0,1 \right] \).

\begin{theorem}[Ionescu-Tulcea for MDMs]
    \label{Ionescu-Tulcea}
    Let \( S,A,D,p,r,\gamma \) be an MDM, \( \pi \) a policy, and \( \mu \) a probability measure on \( S \). Then there exists a unique probability measure \( \mathbb{P}_\mu^\pi \) on \( \Omega, \mathcal{A} \) with the property 
    
    \begin{align*}\label{Ionescu-Tulcea-eq}\tag{\textasteriskcentered}
        \quad &\mathbb{P}_\mu^\pi[B \times \bigtimes_{k = n+1}^{\infty} (S \times A)] \\
         &:= \sum_{(s_{0},a_{0},\mydots,s_n,a_n) \in B} \mu[{s_{0}}] \cdot \pi(a_{0}|s_{0}) \cdot \prod_{k=1}^n p(s_k | s_{k-1}, a_{k-1}) \cdot \pi(a_{k} | s_{k})\\
    \end{align*}



    for all \( B \subseteq (S \times A)^{n+1}, n \in \mathbb{N}_0 \). In particular, it holds that 
    \begin{align*}\label{Ionescu-Tulcea-eq-2}\tag{\textasteriskcentered\textasteriskcentered}
        \quad &\mathbb{P}_\mu^\pi [S_{0}=s_{0}, A_{0} = a_{0}, \mydots, S_n = s_n, A_n = a_n] \\
        &= \mu[{s_{0}}] \cdot \pi(a_{0}|s_{0})\cdot \prod_{k=1}^{n} p(s_k | s_{-k-1}, a_{k-1}) \cdot \pi(a_k | s_k) \\
    \end{align*} 

    for all \( (s_{0},a_{0},s_{1},a_{1},\mydots,s_n, a_n) \in (S \times A)^{n+1}\) and \( n \in \mathbb{N}_{0} \).  \note{This doesnt yet mean that this \( \mathbb{P}_\mu^\pi \) agrees with \( p \) and \( \pi \). But it will be a consequence later.}
\end{theorem}


\begin{proof}[Proof (idea)]
    The key idea is to take \eqref{Ionescu-Tulcea-eq} as a definition and extend it to the full \( \sigma \)-Algebra. Note that the sets of the form 
    \[
        B \times \bigtimes_{k=n+1}^{\infty} (S \times A) 
    \] 
    are an intersection stable ring of sets generating \( \mathcal{A} = \mathcal{P}(\Omega) \). Moreover, it is not hard to show that \( \mathbb{P}_\mu^\pi \) defined by \eqref{Ionescu-Tulcea-eq} is a well-defined additive set function.
    
    With a bit of work, one can show that \( \mathbb{P}_\mu^\pi \) is even \( \sigma \)-additive, hence it is a pre-measure. The existence and uniqueness of \( \mathbb{P}_\mu^\pi \) on all of \( \mathcal{A} \) is thus a direct consequence of Caratheodory's extension theorem.  
    
    Finally, note that 
    \begin{align*}
       &\left\{ S_{0}=s_{0}, A_{0},a_{0},\mydots,S_n = s_n, A_n = a_n \right\} \\
        &= \left\{ (s_{0},a_{0},a_{1},a_{1},\mydots,s_n,a_n) \right\} \times \bigtimes_{k=n+1}^{\infty}(S \times A),\\
    \end{align*}

    so 
    \begin{align*}
        &\mathbb{P}_\mu^\pi \left[ S_{0}=s_{0}, A_{0},a_{0},\mydots,S_n = a_n  \right] \\
        &\overset{\eqref{Ionescu-Tulcea-eq}}{=} \mu[\left\{ s_{0} \right\}] \pi(a_{0},s_{0}) \prod_{k=1}^{n} p(s_k | s_{k-1}, a_{k-1}) \pi(a_{k}|s_{k}).
    \end{align*}
\end{proof}


Having constructed \( \mathbb{P}_\mu^\pi \), it is now straightforward to show that \( \left\{ S_n \right\}_{n \in \mathbb{N}_{0}}, \left\{ A_n \right\}_{n \in \mathbb{N}_{0}} \) have the desired properties under this measure.

\begin{corollary}[Properties of \( \mathbb{P}_\mu^\pi \) ]
    In the setting of \cref{Ionescu-Tulcea}, it holds that
    \begin{enumerate}
        \item \( \mathbb{P}_\mu^\pi[S_{0} = s] = \mu[\left\{ s \right\}], s \in S \)
        \item \( \mathbb{P}_\mu^\pi [S_{n+1} = s' \,|\, S_n = s, A_n = a] = p(s' \,|\, s,a), n \in \mathbb{N}_{0}, s',s \in S, a \in A \)
        \item  \( \mathbb{P}_\mu^\pi[A_{n} = a \,|\, S_{n} = s] = \pi(a | s) \)
        \item \( \begin{aligned}[t]\note{This is the Markov property of the state action pair evolution.}
             & \mathbb{P}_\mu^\pi [ S_{n+1} = s_{n+1}, A_{n+1} = a_{n+1} \,|\, S_{0} = s_{0}, A_{0} = a_{0}, \mydots, S_n = s_n, A_n = a_n] \\
             &\quad =  \mathbb{P}_{\mu}^\pi[S_{n+1} = s_{n+1}, A_{n+1} = a_{n+1} \,|\, S_n = s_n, A_n = a_n]\\[0.2em]
        \end{aligned} \)   for all \( (s_{0},a_{0},\mydots,s_{n+1}, a_{n+1}) \in (S \times A)^{n+2}, n \in \mathbb{N}_{0} \).
        \item \( \mathbb{P}_\mu^\pi[S_{n+1} = s_{n+1} \,|\, S_{0} = s_{0},\mydots, S_n = s_n] = \mathbb{P}_\mu^\pi[ S_{n+1} = s_n \,|\, S_n = s_n]\) 
        for all \( s_{0},s_{1},\mydots,s_{n+1} \in S^{n+1}, n \in \mathbb{N}_{0}\) if \( \pi \) is deterministic \note{That is, if for each \( s \in S \), there exists \( a \in A \) such that \( \pi(a | s) = 1 \).}.  
    \end{enumerate}
\end{corollary}


\begin{proof}[Proof of (ii)]
    
It holds that
\begin{align*}
    &\mathbb{P}_\mu^\pi [S_{n+1} = s', S_n = s, A_n = a] \\
    &= \sum_{a' \in A} \mathbb{P}_\mu^\pi[ S_{n+1} = s', A_{n+1} = a', S_{n} = s, A_{n} = a]\\
    &\overset{\text{\cref{Ionescu-Tulcea}}}{=} \sum_{a' \in A} \sum_{(s_{0},a_{0}),\mydots,(s_{n-1},\mydots,a_{n-1}) \in S \times A} \bigg( \bigg. \mu[\left\{ s_{0} \right\}] \pi(a_{0}| s_{0}) \prod_{k=1}^{n-1} p(s_k | s_{k-1}, a_{k-1}) \pi(a_{k} | s_{k}) \\
    & \cdot p(s | s_{n-1}, a_{n-1}) \pi(a | s) p(s' | s,a) \pi(a' | s')\bigg.\bigg)\\
    &= p(s' | s, a) \sum_{a' \in A} \pi(a' | s') \cdot \sum_{(s_{0},a_{0}),\mydots,(s_{n-1},a_{n-1})} \mu[\left\{ s_{0} \right\}] \pi(a_{0},s_{0}) \prod_{k=1}^{n-1} p(s_{k} | s_{k-1}, a_{k-1}) \pi(a_{k} | s_{k}) p(s | s_{n-1}, a_{n-1}) \pi(a | s) \\
    &= p(s' | s, a) \sum_{a' \in A} \pi(a' | s) \cdot \mathbb{P}_\mu^\pi [S_n = s, A_n = a] \\
    &= p(s' | s, a) \cdot \mathbb{P}_\mu^\pi [ S_n = s , A_n = a].
\end{align*}

Thus 
\begin{align*}
    \mathbb{P}_\mu^\pi &[ S_{n+1} = s' \,|\, S_n = s , A_n = a]\\
    &= \frac{\mathbb{P}_{\mu}^\pi [S_n =s ,A_n = a]}{\mathbb{P}_\mu^\pi[ S_n = s, A_n = a]}\\
    &= p(s' | s, a).
\end{align*}

The rest follows by similar arguments.

\end{proof}



\end{document}