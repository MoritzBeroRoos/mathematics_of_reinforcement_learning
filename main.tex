% !TeX root = main.tex
\input{template/layout.tex}
\input{template/math.tex}
\input{course_specific.tex}





\title{Mathematics of Reinforcement Learning}

\author{Notes by Moritz Roos}
\date{}
%remove page number from tableofcontents page
%\AtBeginDocument{\addtocontents{toc}{\protect\thispagestyle{empty}}}
\begin{document}
\maketitle

\tableofcontents


\clearpage
\section{Introduction}
\textbf{Goals:}
\begin{itemize}
    \item Look at different types of problems.
    \item Learn the basic principles of Reinforcement Learning (RL).
    \item Lean when to apply RL and when not to?
\end{itemize}

\subsection{It's-a me, Mario!}
We want to fix some terminology by the example of the Super Mario game.
\begin{itemize}
    \item \emph{Agent:} player
    \item \emph{Environment:} game
    \item \emph{state:} the frame/screen being presented to the agent
    \item \emph{Action:} input on the controller
    \item \emph{policy:} a correspondence between states and actions
    \item \emph{episode:} one run of the game from start to finish
    \item \emph{reward:} feedback mechanism telling you how good/bad an action performs in a given state
    \item \emph{return:} criterion to be optimized, typically cumulative (discounted, expected) rewards
    \item \emph{state dynamics:} how the next state is obtained from the current state and the current action
\end{itemize}


\begin{figure}[ht]
    \centering
    %\caption{}
    %\incsvg{path/}{path/file}
    \incsvg{figures}{figures/examplemodell}
    \label{fig:examplemodell}
\end{figure}




\section{Markov decision processes}

\textbf{Goals:}
\begin{itemize}
    \item Develop a mathematical framework for dynamic decision making problems under uncertainty.
    \item Lean how to solve these problems if the model is known.
\end{itemize}

\subsection{Definition of markov decision processes}
We look for stochastic processes (a family of random variables indexed by time) \( \{S_n\}_{n \in  \mathbb{N}_0 }, \{A_n\}_{n \in \mathbb{N}_{0}} \{R_n\}_{n \in \mathbb{N}_{0}} \) modelling the dynamic evolution of states, actions and rewards.

\begin{definition}[Markov decision model]
    A \emph{Markov Decision Model} is a tuple \( (S, A, D, p, r ,\gamma) \) consisting of the following components:
    \begin{enumerate}
        \item a finite set \( S \)  called \emph{state space}.
        \item a finite set \( A \) called \emph{action space}
        \item a set \( D \subseteq S \times A \) whose elements are the \emph{admissible state-action pairs}
        \item a \emph{transition probability function} \( p: S \times S \times A \implies [0,1] \)
              \[
                  (s', s, a) \mapsto p(s' | s, a)\note{This is the probability of ending up in \( s' \) when performing action \( a \) in state \( s \). \( s' \mapsto p(s'| s,a) \) is a probability mass function.}
              \] satisfying
              \[
                  \sum_{s' \in S}p(s' | s, a) = 1
              \] for all \( (s,a) \in S \times A \).
        \item a \emph{reward function} \( r: D \to \mathbb{R} \)
              \[
                  (s,a) \mapsto r(s,a)\note{The reward for performing action \( a \)  in state \( s \).}
              \]
        \item a \emph{discount factor} \( \gamma \in (0,1] \).\note{This encodes the time-value of rewards. \( \gamma^{-1}\cdot r(s,a) \) is the value at time \( 0 \) of receiving \( r(s,a) \) \( n \) steps into the future.}

    \end{enumerate}
\end{definition}


\begin{definition}[Policy \qquad \( \Pi \)  \qquad \( \Pi_d \) ]\label{def_policy}
    A \emph{policy} is a mapping \( \pi: S \times A \to [0,1] \)
    \[
        (s,a) \mapsto \pi(a|s)
    \]
    such that
    \[
        \sum_{a \in A} \pi(a|s) = 1 \note{This means that \( a \mapsto \pi(a|s) \) is a probability mass function for each fixed \( s \) . Thus one action \textit{has} to be chosen.}
    \] for all \( s \in S \) and
    \[
        \pi(a|s) = 0
    \] for all \( s,a \in S\times A \setminus D\).

    We say that \( \pi \) is a \emph{deterministic policy} if
    \[
        \fa{s \in S:}\ex{a \in A:}\pi(a|s) = 1.
    \] We write \( \Pi \) and \( \Pi_d \) for the set of all policies and the subset of deterministic policies respectively.

\end{definition}

To wit, \( \pi(a|s) \) is interpreted as the probability of choosing action \( a \) in state \( s \). One could also introduce \quotes{non-Markovian} policies which depend on the entire history of states and actions or \quotes{non-stationary} policies which depend on the current state and current time.
However, we spare ourselves the trouble, since we will eventually see that there always is a \quotes{stationary, Markovian} optimal policy as in \cref{def_policy}.


\ \\
Intuitively, what happens for a given MDM and a policy \( \Pi \) is the following:

\begin{itemize}
    \item Start in an initial state \( S_{0}:= s_{0} \).
    \item Using the policy \( \pi \), randomly draw an action \( A_{0} \) from \( a \mapsto \pi(a|S_{0}) \).
    \item Collect the reward \( R_{0} := r(S_{0}, A_{0})\) and draw the next state \( S_{1} \) from \( s' \mapsto p(s'|S_{0}, A_{0}) \).
    \item repeat this procedure to construct \( S_{0}, A_{0}, R_{0}, S_{1}, A_{1}, R_{1},\mydots \)
\end{itemize}

Informally, we can then introduce the objective
\[
    \mathbb{E} \left[ \sum_{n=0}^{\infty} \gamma^{n} \cdot R_n \right] = \mathbb{E} \left[ \sum_{n=0}^{\infty} \gamma^{n} \cdot r(S_{n}, A_{n}) \right].
\]


Now we want to construct the mentioned stochastic processes. They should be consistent with our definition of MDM's and policies.\note{This can be seen as the formalization of the intuitive description of what should happen for a given MDM and a policy.} This enables us to make sense of notions like the expected return mentioned above.

For this, the first ingredient is a probability space. We choose the sample space as 
\[
    \Omega := (S \times A)^\infty = \bigtimes_{n=0}^{\infty} (S \times A)
\]
and let the \( \sigma \)-algebra \( \mathcal{A} \) on \( \Omega \) be the power set \( \mathcal{P}(\Omega) \) of \( \Omega \). We can safely do so, since \( \Omega \) is countable. In fact, any element \( w \in \Omega  \) takes the form 
\[
    \omega = (s_{0},a_{0},s_{1},a_{1},s_{2},a_{2},\mydots)
\]  
where \(  \left\{ s_n \right\}_{n \in \mathbb{N}_{0}} \subseteq S, \left\{ a_n \right\}_{n \in \mathbb{N}_{0}} \subseteq A \). With that in mind, we define functions
\[
    S_n(\omega) = S_n((s_{0},a_{0},s_{1},a_{1},\mydots)) := s_n
\]
\[
    A_n(\omega) = A_n((s_{0},a_{0},s_{1},a_{1},\mydots)) := a_n  
\]
for \( n \in \mathbb{N}_{0}, \omega \in \Omega \). 

Then \( S_n, A_n \) trivially\note{Trivially, since we chose the power set as \( \sigma \)-algebra. } are measurable functions ( = random variables) taking values in \( S, A \) respectively.  What is left is showing that there exists a probability measure \( \mathbb{P}^\pi_\mu \) on \( (\Omega, \mathcal{A}) \) such that 
\( S_{0},A_{0}, S_{1},A_{1},\mydots \) have the desired distribution.\note{The \( A \) 's should be determined by the policy and the last state. The \( S \) 's by the transition function and the taken action.}
This can be constructed from \( p, \pi \)  and an initial distribution \( \mu \) on \( S \) such that \( S_{0} \sim \mu \).

What we would like to do is define
\begin{align*}
    \mathbb{P}_\mu^\pi [\left\{ \omega \right\}] &= \mathbb{P}_\mu^\pi[{(s_{0},a_{0},s_{1},a_{1},s_{2},a_{2},\mydots)}] \\
    &:= \mu[\left\{ s_{0} \right\}] \cdot \pi(a_{0} | s_{0}) \cdot p(s_{1} | s_{0},a_{0}) \cdot \pi(a_{1} | s_{1}) \cdot p(s_{2}|s_{1},a_{1}) \cdot \pi(a_{2}|s_{2}) \cdot \mydots \\
    &= \mu[\left\{ s_{0} \right\}] \pi(a_{0},s_{0}) \cdot \prod_{n=1}^\infty p(s_n | s_{n-1}, a_{n-1}) \pi(a_n | s_n) \\
\end{align*}
for all \( \omega = (s_{0},a_{0},s_{1},a_{1},\mydots) \in \Omega \).
The problem is, however, that the resulting probability is usually zero, as this is an infinite product with factors valued in \( \left[ 0,1 \right] \).

\begin{theorem}[Ionescu-Tulcea for MDMs]
    \label{Ionescu-Tulcea}
    Let \( S,A,D,p,r,\gamma \) be an MDM, \( \pi \) a policy, and \( \mu \) a probability measure on \( S \). Then there exists a unique probability measure \( \mathbb{P}_\mu^\pi \) on \( \Omega, \mathcal{A} \) with the property 
    
    \begin{align*}\label{Ionescu-Tulcea-eq}\tag{\textasteriskcentered}
        \quad &\mathbb{P}_\mu^\pi[B \times \bigtimes_{k = n+1}^{\infty} (S \times A)] \\
         &:= \sum_{(s_{0},a_{0},\mydots,s_n,a_n) \in B} \mu[{s_{0}}] \cdot \pi(a_{0}|s_{0}) \cdot \prod_{k=1}^n p(s_k | s_{k-1}, a_{k-1}) \cdot \pi(a_{k} | s_{k})\\
    \end{align*}



    for all \( B \subseteq (S \times A)^{n+1}, n \in \mathbb{N}_0 \). In particular, it holds that 
    \begin{align*}\label{Ionescu-Tulcea-eq-2}\tag{\textasteriskcentered\textasteriskcentered}
        \quad &\mathbb{P}_\mu^\pi [S_{0}=s_{0}, A_{0} = a_{0}, \mydots, S_n = s_n, A_n = a_n] \\
        &= \mu[{s_{0}}] \cdot \pi(a_{0}|s_{0})\cdot \prod_{k=1}^{n} p(s_k | s_{k-1}, a_{k-1}) \cdot \pi(a_k | s_k) \\
    \end{align*} 

    for all \( (s_{0},a_{0},s_{1},a_{1},\mydots,s_n, a_n) \in (S \times A)^{n+1}\) and \( n \in \mathbb{N}_{0} \).  \note{This doesnt yet mean that this \( \mathbb{P}_\mu^\pi \) agrees with \( p \) and \( \pi \). But it will be a consequence later.}
\end{theorem}


\begin{proof}[Proof (idea)]
    The key idea is to take \eqref{Ionescu-Tulcea-eq} as a definition and extend it to the full \( \sigma \)-Algebra. Note that the sets of the form 
    \[
        B \times \bigtimes_{k=n+1}^{\infty} (S \times A) 
    \] 
    are an intersection stable ring of sets generating \( \mathcal{A} = \mathcal{P}(\Omega) \). Moreover, it is not hard to show that \( \mathbb{P}_\mu^\pi \) defined by \eqref{Ionescu-Tulcea-eq} is a well-defined additive set function.
    
    With a bit of work, one can show that \( \mathbb{P}_\mu^\pi \) is even \( \sigma \)-additive, hence it is a pre-measure. The existence and uniqueness of \( \mathbb{P}_\mu^\pi \) on all of \( \mathcal{A} \) is thus a direct consequence of Caratheodory's extension theorem.  
    
    Finally, note that 
    \begin{align*}
       &\left\{ S_{0}=s_{0}, A_{0},a_{0},\mydots,S_n = s_n, A_n = a_n \right\} \\
        &= \left\{ (s_{0},a_{0},a_{1},a_{1},\mydots,s_n,a_n) \right\} \times \bigtimes_{k=n+1}^{\infty}(S \times A),\\
    \end{align*}

    so 
    \begin{align*}
        &\mathbb{P}_\mu^\pi \left[ S_{0}=s_{0}, A_{0},a_{0},\mydots,S_n = a_n  \right] \\
        &\overset{\eqref{Ionescu-Tulcea-eq}}{=} \mu[\left\{ s_{0} \right\}] \pi(a_{0},s_{0}) \prod_{k=1}^{n} p(s_k | s_{k-1}, a_{k-1}) \pi(a_{k}|s_{k}).
    \end{align*}
\end{proof}


Having constructed \( \mathbb{P}_\mu^\pi \), it is now straightforward to show that \( \left\{ S_n \right\}_{n \in \mathbb{N}_{0}}, \left\{ A_n \right\}_{n \in \mathbb{N}_{0}} \) have the desired properties under this measure.

\begin{corollary}[Properties of \( \mathbb{P}_\mu^\pi \) ]
    In the setting of \cref{Ionescu-Tulcea}, it holds that
    \begin{enumerate}
        \item \( \mathbb{P}_\mu^\pi[S_{0} = s] = \mu[\left\{ s \right\}], s \in S \)
        \item \( \mathbb{P}_\mu^\pi [S_{n+1} = s' \,|\, S_n = s, A_n = a] = p(s' \,|\, s,a), n \in \mathbb{N}_{0}, s',s \in S, a \in A \)
        \item  \( \mathbb{P}_\mu^\pi[A_{n} = a \,|\, S_{n} = s] = \pi(a | s) \)
        \item \( \begin{aligned}[t]\note{This is the Markov property of the state action pair evolution.}
             & \mathbb{P}_\mu^\pi [ S_{n+1} = s_{n+1}, A_{n+1} = a_{n+1} \,|\, S_{0} = s_{0}, A_{0} = a_{0}, \mydots, S_n = s_n, A_n = a_n] \\
             &\quad =  \mathbb{P}_{\mu}^\pi[S_{n+1} = s_{n+1}, A_{n+1} = a_{n+1} \,|\, S_n = s_n, A_n = a_n]\\[0.2em]
        \end{aligned} \)   for all \( (s_{0},a_{0},\mydots,s_{n+1}, a_{n+1}) \in (S \times A)^{n+2}, n \in \mathbb{N}_{0} \).
        \item \( \mathbb{P}_\mu^\pi[S_{n+1} = s_{n+1} \,|\, S_{0} = s_{0},\mydots, S_n = s_n] = \mathbb{P}_\mu^\pi[ S_{n+1} = s_n \,|\, S_n = s_n]\) 
        for all \( s_{0},s_{1},\mydots,s_{n+1} \in S^{n+1}, n \in \mathbb{N}_{0}\) if \( \pi \) is deterministic \note{That is, if for each \( s \in S \), there exists \( a \in A \) such that \( \pi(a | s) = 1 \).}.  
    \end{enumerate}
\end{corollary}


\begin{proof}[Proof of (ii)]
    
It holds that
\begin{align*}
    &\mathbb{P}_\mu^\pi [S_{n+1} = s', S_n = s, A_n = a] \\
    &= \sum_{a' \in A} \mathbb{P}_\mu^\pi[ S_{n+1} = s', A_{n+1} = a', S_{n} = s, A_{n} = a]\\
    &\overset{\text{\cref{Ionescu-Tulcea}}}{=} \sum_{a' \in A} \sum_{(s_{0},a_{0}),\mydots,(s_{n-1},\mydots,a_{n-1}) \in S \times A} \bigg( \bigg. \mu[\left\{ s_{0} \right\}] \pi(a_{0}| s_{0}) \prod_{k=1}^{n-1} p(s_k | s_{k-1}, a_{k-1}) \pi(a_{k} | s_{k}) \\
    & \cdot p(s | s_{n-1}, a_{n-1}) \pi(a | s) p(s' | s,a) \pi(a' | s')\bigg.\bigg)\\
    &= p(s' | s, a) \sum_{a' \in A} \pi(a' | s') \cdot \sum_{(s_{0},a_{0}),\mydots,(s_{n-1},a_{n-1})} \mu[\left\{ s_{0} \right\}] \pi(a_{0},s_{0}) \prod_{k=1}^{n-1} p(s_{k} | s_{k-1}, a_{k-1}) \pi(a_{k} | s_{k}) p(s | s_{n-1}, a_{n-1}) \pi(a | s) \\
    &= p(s' | s, a) \sum_{a' \in A} \pi(a' | s) \cdot \mathbb{P}_\mu^\pi [S_n = s, A_n = a] \\
    &= p(s' | s, a) \cdot \mathbb{P}_\mu^\pi [ S_n = s , A_n = a].
\end{align*}

Thus 
\begin{align*}
    \mathbb{P}_\mu^\pi &[ S_{n+1} = s' \,|\, S_n = s , A_n = a]\\
    &= \frac{\mathbb{P}_{\mu}^\pi [S_{n+1} = s, S_n =s ,A_n = a]}{\mathbb{P}_\mu^\pi[ S_n = s, A_n = a]}\\
    &= p(s' | s, a).
\end{align*}

The rest follows by similar arguments.

\end{proof}


Given \( s \in S \), we write \( \delta_s \) for the Dirac measure with mass on \( s \), that is 
\[
    \delta_s[\left\{ s \right\}] = 1, \quad \delta_s[\left\{ s' \right\}] = 0, s' \in S \setminus \left\{ s \right\}
\]
For any \( \pi \in \Pi  \), we then write \( \mathbb{P}_s^\pi := \mathbb{P}_{\delta_s}^\pi\) for the expectation operator with respect to \( \mathbb{P}_\mu^\pi \) and let \( \mathbb{E}_s^\pi = \mathbb{E}_{\delta_s}^\pi \).
 


\subsection{Value Functions and bellman Equations}

\begin{definition}[\( V^\pi \qquad V\) ]
    For any \( \pi \in \Pi \), the function \( V^\pi: S \to \mathbb{R} \),
    \[ s \mapsto V^\pi(s) := \mathbb{E}_s^\pi\left[ \sum_{n=0}^{\infty}\gamma^n r(S_n, A_n) \right] \]
    is called the \emph{state value function} (if it exists).
    Moreover, if it exists, the function \( V: S \to \mathbb{R} \), 
    \[
        s \mapsto V(s) := \sup_{\pi \in \Pi} V^\pi (s) = \sup_{\pi \in \Pi}\, \mathbb{E}_{s}^{\pi}\left[ \sum_{n=0}^{\infty} \gamma^n r(S_n, A_n) \right]
    \] 
    is called \emph{(optimal) value function}.

    Finally, a policy \( \pi^\star \in \Pi \) such that \( V^{\pi^\star} := V(s) \) is called \emph{optimal} for the initial state \( s \in S \). 

\end{definition}

\begin{remark}
    \( V^\pi \) and \( V \) exist under very mild conditions indeed. Since \( S \) and \( A \) are finite, we conclude that \( |r| \) is bounded. A sufficient condition for the existence of \( V^\pi \) and \( V \) is hence \( \gamma < 1  \) as 
    \[
        \sum_{n=0}^{\infty} \gamma^n | r(S_n, A_n)| \leq \max_{(s,a) \in D} | r(a,s) | \sum_{n=0}^{\infty} \gamma^n = \frac{1}{1-\gamma} \max_{(s,a) \in D} |r(s,a)| \text{a.s.}
    \]        
    for all \( \mathbb{P}_s^\pi \). If the MDM has a finite time horizon, meaning that there is \( N \in \mathbb{N}_{0} \) s.t. \( r(S_n, A_n) = 0 \fa{n \geq N}  \mathbb{P}_s^\pi-\text{a.s.}\) for all \( s \in S,  \in \Pi \), we do not have to assume \( \gamma < 1 \).

\end{remark}

\emph{Standing Assumption:} There exists some \( C > 0 \) s.t. 
\[
    \sum_{n=0}^{\infty} \gamma^n | r(S_n, A_n)| \leq C\quad  \mathbb{P}_s^\pi-\text{a.s.}
\] 
for all \(  \in \Pi, s \in S\). 
While it may seem odd that we are now working with an entire family of optimization problems \( \left\{ V(s) \right\}_{s \in S} \), it is not hard to imagine, that these problems are intimately related and we can learn somethig from that connection.

\begin{lemma}[Time shift]
    \label{time_shift_lemma}
    Let \( s,s' \in S, a \in A \), and \( \pi \in \Pi \).
    Define 
    \[
        \mathbb{P}_{s,a,s'}^\pi := \mathbb{P}_s^\pi[ \cdot \,|\, S_{0} = s, A_{0} = a, S_{1} = s']
    \] 
    and 
    \[
        \hat{S_n} := S_{n+1}, \hat{A}_n := A_{n+1}, n \in \mathbb{N}_{0}
    \]
    then the distribution of \( \left\{ (\hat{S}_n, \hat{A}_n) \right\}_{n \in \mathbb{N}_{0}} \) under \( \mathbb{P}_{s,a,s'}^\pi \) is the same as the distribution of 
    \( \left\{ (S_n, A_n) \right\}_{n \in \mathbb{N}_{0}} \) under \( \mathbb{P}_{s'}^\pi \).    
\end{lemma}

\begin{proof}
    For \(  n  \in \mathbb{N}_{0}, (s_{0},a_{0},s_{1},a_{0},\mydots,s_n,a_n) \in (S\times A)^{n+1} \), it suffices to show

    \[
        \mathbb{P}_{s,a,s'}^\pi [ \hat{S}_0 = s_{0}, \hat{A}_{0} = a_{0}, \mydots, \hat{S}-n = s_n, \hat{A}_n = a_n] = \mathbb{P}_{s'}^\pi [S_0 = s_{0}, A_{0} = a_{0}, \mydots, S_n = s_n, A_n = a_n].
    \]
    The identity is immediate from the definitions, if \( s_{0} \neq s' \) since both sides are zero in that case. So assume further that \( s_{0} = s' \).  
    Then we have

    \begin{align*}
        &\mathbb{P}_{s,a,s'}^\pi[\hat{S}_0 = s', \hat{A}_{0} = a_{0}, \mydots, \hat{S}_n = s_n, \hat{A}_n = a_n] \\
        &= \mathbb{P}_s^\pi[ \hat{S}_{0} = s', \hat{A}_{0} = a_{0}, \mydots, \hat{S}_{0} = s_n, \hat{A}_n = a_n \,|\, S_{0} = s, A_{0} = a, S_{1} = s'] \\
        &= \mathbb{P}_s^\pi [S_{0} =s, A_{0}= a, S_1 = s', A_{1} = a_{0}, \mydots, S_{n+1} = s_n, A_{n+1} = a_n] / \mathbb{P}_s^\pi[S_0 = s, A_{0} = a, s_n = s']\\
        &= \frac{\delta_s[\left\{ s \right\}] \pi(a|s) p(s' | s,a) \pi(a_{0}|s') \prod_{k=1}^n p(s_k | s_{k-1}, a_{k-1}) \pi(a_k | s_k)}{\delta_s[\left\{ s \right\}] \pi(a | s) p(s' | s,a)} \\
        &= \delta_{s'}[\left\{ s' \right\}] \pi(a_{0} | s') \prod_{k=1}^n p(s_k | s_{k-1}, a_{k-1}) \pi(a_k, s_k) = \mathbb{P}_{s'}^\pi[ S_{0} = s', A_{0} = a_{0}, \mydots, S_n = s_n, A_n = a_n]\\ 
    \end{align*}
\end{proof}


\begin{theorem}[Bellman Equation]
    Let \( s \in S, \pi   \). Then 
    \begin{align*}
                V^\pi(s) &= \sum_{s' \in S, a \in A} \left[ r(s,a) + \gamma V^\pi(s')  \right] \pi(a | s) p(s' | s,a)\\
                 &= \mathbb{E}_s^\pi \left[ r(S_{0},A_{0}) + \gamma V^\pi(s_{1}) \right] \\
                 \bigg(&= \sum_{a \in A} \left[ r(s,a) + \gamma \sum_{s' \in S} p(s' | s,a) V^\pi(s')\right] \pi(a | s)\bigg) \note{The part in square brackets will get its own name in the next definition.}
    \end{align*}
\end{theorem}
\begin{proof}
    We have 
    \begin{align*}
        V^\pi(s) &= \mathbb{E}_s^\pi \left[  \sum_{n=0}^{\infty}\gamma^n r(S_n, A_n) \right] \\
        &= \sum_{s' \in S, a\in A} \mathbb{E}_s^\pi \left[ r(S_{0},A_{0}) + \gamma\cdot \sum_{n=1}^\infty \gamma^{n-1} r(S_n, A_n)    \,|\, S_{0} = s, A_{0} = a, S_{1} = s' \right] \cdot \mathbb{P}_s \left[ S_{0} = s, A_{0} = a, S_{1} = s' \right]\\
        &= \sum_{s' \in s, a \in A} \left[ r(s,a) + \gamma \mathbb{E}_s^\pi \left[ \sum_{n=1}^\infty \gamma^{n-1} r(S_n, A_n) | S = s, A_{0} = a, S_{1} = s' \right] \right] \pi(a|s) p(s'| s,a) \\
        &= \sum_{s' \in S, a \in A }\left[ r(s,a) + \gamma \mathbb{E}_{s'}^\pi \left[ \sum_{n=0}^{\infty} \gamma^n r(S_n, A_n) \right] \right] \\
        &= \sum_{s' \in S, a \in A} \left[ r(s,a) + \gamma V^\pi(s') \right] \pi(a| s) p(s' | s,a)
    \end{align*}
    where \( \sum_{n=1}^{\infty} \gamma^{n-1} r(S_n, A_n)  = \sum_{n=0}^{\infty} \gamma^n r(\hat{S}_n, \hat{A}_n) \sim \sum_{n=0}^{\infty} \gamma^n r(S_n, A_n)\) under \( \mathbb{P}_{s'}^\pi \) by \cref{time_shift_lemma}.  
\end{proof}


\begin{definition}[\( Q^\pi \) \qquad \( Q \) ]
    For any \( \pi \in \Pi \), we refer to \( Q^\pi: S \times A \to \mathbb{R} \),
    \[
        (s,a) \mapsto Q^\pi(s|a) := r(s,a) + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot V^\pi(s')
    \]  
    as the \emph{action value function} for the policy \( \pi \). \\
    
    Similarly we call \( Q: S \times A \to R \), 
    \[  
        (s,a) \mapsto Q(s|a) := r(s,a) + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot V(s')
    \]
    the \emph{optimal action value function}.
\end{definition}

Clearly
\[
    V^\pi(s) = \sum_{a \in A} Q^\pi (s|a) \cdot \pi(a|s) 
\]
and we \textit{expect} 
\[
    V(s) = \max_{a \in A(s)} Q(s|a),
\]
where 
\(  A(s) := \left\{  a \in A: (s,a) \in D \right\} \) are the admissible actions in state \( s \in S\).

Since 
\[  
    V^\pi(s) = \sum_{s' \in S, a \in A} \left[ r(s,a) + \gamma \cdot V^\pi(s') \right] \pi(a|s) \cdot p(s' | s,a)
\] for \( s \in S \) we see that \( V^\pi \) can be computed by solving a linear system of equations\note{This really is feasible in practice, if the state space is small enough!}. 

Similarly, we have
\[  
    Q^\pi(s|a) = r(s,a) + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot \sum_{a' \in A} \pi(a' | s') \cdot Q^\pi (s' | a') 
\]
for all \( (s,a) \in S \times A \). 
If \( \pi^* \) is optimal, it clearly also satisfies the Bellman equation. 
It is therefore not surprising, that there is a variant of the Bellman equation for the optimal value function \( V \).
For any \( \pi \in \Pi \), we have 

\begin{align*}
      V^\pi(s) &= \sum_{a' \in A(s)} \left[ r(s, a') + \gamma \cdot \sum_{s' \in S}p(s'| s, a') \cdot V^\pi(s') \right] \cdot \pi(a' | s) \note{If \( a' \notin A(s) \) then \( \pi(a') = 0 \). }   \\
      &\leq \max_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{ s' \in S} p(s' | s,a) \cdot V^\pi(s') \right] \cdot \sum_{a' \in A(s)} \pi(a' | s)\\
      &\leq \max_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{ s' \in S} p(s' | s,a) \cdot V(s') \right] \cdot \sum_{a' \in A(s)} \pi(a' | s)\\
      &= \max_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{ s' \in S} p(s' | s,a) \cdot V(s') \right] \cdot 1
\end{align*}
for any \( \pi \).
So also 
\[
    V(s) = \max_{\pi \in \Pi} V^\pi(s) \leq  \max_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{ s' \in S} p(s' | s,a) \cdot V(s') \right].
\]

The reverse inequality will now only be derived heuristically:
Suppose that we allow non-stationary policies \( \pi_a^\epsilon \) which first choose a fixed action \( a \in A(s) \) and for any \( s' \in S \) and fixed \( \epsilon > 0 \) follow an \( \epsilon \)-optimal policy \( \pi^{s'} \), meaning \( V^{\pi^{s'}} \geq V(s') - \epsilon \).
The Bellman equation then suggests that \note{We haven't actually proven that it also holds for non-stationary policies.} 
\begin{align*}
     V(s) &\geq V^{\pi_a^\epsilon}(s)\\
          &= \sum_{a' \in A} \left[ r(s, a') + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot V^{\pi_a^\epsilon}(s) \right] \cdot \pi(a'|s) 
          \note{\( V^{\pi_a^\epsilon}(s') = V^{\pi^{s'}}(s') \) and \( \pi(a'|s) = 1 \) iff \( a' = a \)   }\\
          &= r(s,a) + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot V^{\pi^{s'}}(s') \\
          &\geq r(s,a) + \gamma \cdot \sum_{s' \in S} p(s'|s,a) \left[ V(s')-\epsilon \right]
\end{align*}
for arbitrary \( a \in A(s)  \) and \( \epsilon > 0 \).  
so
\[
    V(s) \geq \max_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{s' \in S} p(s'|s,a) \cdot V(s') \right].
\]









\begin{theorem}[Bellman Optimality Equation]
Let \( W: S \to \mathbb{R}  \) satisfy the \emph{Bellman optimality equation},
i.e. 
\[
    W(s) = \max_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{s' \in S} p(s'|s,a) \cdot W(s') \right].
\]
Let moreover \( \pi^* \in \Pi \) be a policy with the property
\[
    \pi^*(a|s) > 0 \implies a \in \argmax_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot W(s') \right].
\]
If in addition 
\[
    \lim_{n \to \infty} \gamma^n W(S_n) = 0 \,\, \mathbb{P}_s^{\pi^*}\text{-a.s.} \fa{s \in S}
\]
and 
\[
    \limsup_{n \to \infty} \gamma^n W(S_n) \geq 0 \,\, \mathbb{P}_s^\pi\text{-a.s.} \fa{s \in S, \pi \in \Pi}, \]
    then \( \pi^* \) is optimal for \textit{all} initial states \( s \in S \) and \( W = V \).   
\end{theorem}
\begin{proof}
For any \( s \in S  \) and \( \pi \in \Pi \), we have 
\begin{align*}
    W(s) &= \max_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{s' \in S} p(s_1 | s, a_{0}) \cdot W(s_{1}) \right] \\
    &\geq \note{Here we have equality if \( \pi = \pi^* \). }\sum_{a_{0} \in A} \left[ r(s, a_{0}) + \gamma \cdot \sum_{s_{1} \in S}p(s_{1} | s,a_{0}) \cdot W(s_{1})  \right] \cdot \pi(a_{0} | s)\\
    &= \sum_{(s_{0},a_{0}) \in S \times A} \left[ r(s_{0},a_{0}) + \gamma \cdot W(s_{1}) \right] \cdot \delta_s[\left\{ s_{0} \right\}] \cdot \pi(a_{0}|s_{0}) \cdot p(s_{1} | s_{0}, a_{0})\note{Now we can use the equation we derived until this point again for \( s = s_{1} \). }\\
    &\geq \sum_{(s_{0},a_{0}) \in S \times A} \left[ r(s_{0},a_{0}) + \gamma \cdot \left( \sum_{a_{1} \in A, s_{2} \in S}(r(s_{1},a_{1}) + \gamma \cdot W(s_2)) \cdot \pi(a_{1}|s_{1}) \cdot p(s_{2}| s_{1},a_{1}) \right) \right] \\
    & \qquad \qquad \qquad \cdot \delta_s[\left\{ s_{0} \right\}] \cdot \pi(a_{0}|s_{0}) \cdot p(s_{1} | s_{0}, a_{0})\\
    &= \sum_{(s_{0},a_{0}),(s_{1},a_{1}) \in S \times A, s_{2} \in S} \left[ r(s_{0},a_{0}) + \gamma \cdot r(s_{1},a_{1}) + \gamma^2 \cdot W(s_2) \right]\\
    &\qquad \qquad \qquad \cdot \delta_s[\left\{  s_{0} \right\}] \cdot \pi(a_{0}|s_{0}) \cdot p(s_{1}| s_{0},a_{0}) \cdot \pi(a_{1} | s_{1}) \cdot p(s_{2} | s_{1}, a_{1})\note{Now we can apply the equation again for \( W(s_2) \) etc\mydots }\\
    &\geq \mydots \\
    &\geq \sum_{(s_{0},a_{0}),\mydots,(s_n,a_n) \in S \times A, s_{n+1} \in S} \left[ \sum_{k=0}^n \gamma^k \cdot r(s_k, a_k) + \gamma^{n+1} \cdot W(s_{n+1}) \right]\\
    &\qquad \qquad \qquad \cdot \delta_s[\left\{  s_{0} \right\}] \cdot \pi(a_{0}|s_{0}) \cdot \prod_{k=1}^n p(s_k | s_{k-1}, a_{k-1}) \cdot \pi(a_{k}| s_{k}) \cdot p(s_{n+1}|s_n, a_n)\\
    &= \mathbb{E}_s^\pi \left[  \sum_{k=0}^{n} \gamma^k \cdot r(S_k, A_k) + \gamma^{n+1} W(S_{n+1}) \right] \\
\end{align*}
which converges against something which by dominated convergence is larger than 
\[
    \mathbb{E}_s^\pi \left[ \sum_{n=0}^{\infty} \gamma^n \cdot r(S_n, A_n) \right] = V^\pi(s).
\]
Similarly for \( \pi = \pi^* \) we get equalities \textit{everywhere}, so \( W(s) = V^{\pi^*}(s). \)
Hence
\[
    V^{\pi^*} = W \geq \sup_{\pi \in \Pi} V^\pi = V \geq V^{\pi^*},
    \]
so 
\[
    W = V = V^{\pi^*},
\]
thus \( \pi^*  \) is optimal for all states \( s \in S \).    
\end{proof}


\begin{theorem}[Existence of a fixed point]
    Assume \( \gamma < 1  \) and define an operator \( \mathcal{T} \) acting on functions 
    \( w: s \to \mathbb{R} \) via 
    \[
        \mathcal{T}[w](s) := \max_{a \in A(s)}\left[ r(s,a) + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot w(s') \right]
    \]
    Then \( \mathcal{T} \) has a unique fixed-point \( W: S \to \mathbb{R} \), that is
    \[
        W(s) = \mathcal{T}[W](s)
    \]
    for all \( s \in S \).\note{That means it solves the bellman optimality equation! This whole Theorem is just a rephrasing of the existence of a solution to a fixed-point problem.}
\end{theorem}
\begin{proof}
    Exercise. Idea: Use Banach fixed-point theorem for the contraction \( \mathcal{T} \) (with rate \( \gamma \)) on some suitable space of functions with some suitable norm.
\end{proof}



With the previous theorem, we now have essentially our first algorithm to solve MDP's numerically:

Starting from an arbitrary initial guess \( w_{0}: S \to \mathbb{R} \) for the value function define for all \( n \in \mathbb{N} \)
\[
    w_n := \mathcal{T}[w_{n-1}].
\]
By Banachs fixed-point theorem, if \( \gamma < 1\) this sequence of functions converges to the value function \( V \), the unique fixed point of \( \mathcal{T} \).
An (approximate) optimal policy is obtained by choosing for each \( s \in S \), \textit{any} maximizer \( a^*(s) \in A(s) \) in \( \mathcal{T}[V](s) \), that is 
\[
    a^*(s) \in \argmax_{a \in A(s)} \left[ r(s,a) + \gamma \cdot \sum_{s' \in S} P(s' | s,a) \cdot V(s') \right].
\]

In particular, there always is a \emph{deterministic} optimal policy.














\subsection{Policy improvement and policy Iteration}

The fixed point argument developed in the last section iteratively computes an approximation of \( V \) to solve the MDP.

In this sectionm we instead develop an iterative Method which works on the level of policies.The key to constructing such an iteration is the following theorem.
Standing Assumption for this section:

\[
    \lim_{n \to \infty} \gamma^n V^{\widetilde{\pi}}(S_n) = 0 \,\, \mathbb{P}_s^\pi \text{-a.s.} \text{for all } \pi, \widetilde{\pi} \in \Pi, s \in S
\]


\begin{theorem}[Policy improvement]
    Let \( \pi, \pi' \in \Pi \) such that for all \( s \in S \) we have
    \begin{equation}
        \sum_{a \in A} \pi(a|s) Q^\pi(s|a) \leq \sum_{a \in A} \pi'(a|s) Q^\pi(s|a). \tag{\textasteriskcentered}\label{policy_improvement_eq}
    \end{equation}
    Then \( \pi' \) outperforms \( \pi \), that is 
    \[
        V^\pi \leq V^{\pi'}.
    \]
    Moreover if the inequality in \eqref{policy_improvement_eq} is strict in some state \( s_{0} \), then \( V^\pi(s_{0}) < V^{\pi'}(s_{0}) \).
\end{theorem}

\begin{remark}
The right hand side of \eqref{policy_improvement_eq} can be interpreted as following the policy \( \pi' \) in the first time step, and then switching to \( \pi \). If this perfoms better than choosing \( \pi \) also for the first step, then \( \pi' \) is better overall.     
\end{remark}


\begin{proof}
    Let \( s \in S \). 
    Have 
    \begin{align*}
        V^\pi(s) &= \sum_{a \in A} \pi(a|s) \cdot Q^\pi(s|a) \\
               &\leq \sum_{a \in A} \pi'(a|s) Q^\pi(s|a) \\
               &=\note{by Definition of \( Q^\pi \)} \sum_{a \in A} \pi'(a|s) \cdot \left[ r(s,a) + \gamma \cdot \sum_{s' \in S} p(s' | s,a) \cdot V^\pi(s') \right]\\
               &= \sum_{(s_{0},a_{0}) \in S \times A, s_{1} \in S} \left[ r(s_{0},a_{0}) + \gamma \cdot V^\pi(s) \right] \cdot \delta_s[s_{0}] \cdot \pi'(a_{0}|s_{0}) \cdot p(s_{1} | s_{0},a_{0}) \\
               &= \mathbb{E}_s^{\pi'} \left[ r(S_{0},A_{0}) + \gamma \cdot V^\pi(S_{1})\right]\\
               &\leq\mydots \note{Now we can use the until now already proven inequality again (\( N -1\) times), but for \( V^\pi(S_{1}) \) instead of for ??} \\
               &\leq \mathbb{E}_s^{\pi'}\left[ \sum_{n=0}^N \gamma^n \cdot r(S_n, A_n) + \gamma^{N+1} \cdot V^\pi(S_{N+1}) \right]              \\ 
    \end{align*}    
    for any \( N \in \mathbb{N} \).
    So 
    \[
        V^\pi(s) \leq \mathbb{E}_s^{\pi'} \left[ \sum_{n=0}^{\infty} \gamma^n \cdot r(S_n, A_n) \right]
    \]
    by limiting both sides and using dominated convergence to exchange limit with Expectation.

    If the inequality in \eqref{policy_improvement_eq} is strict at \( s_{0} \), then all the our first inequality becomes strict. This strictness carries through to the end.
\end{proof}


The previous Theorem gives an easy way of improving a given policy \( \pi \in \Pi \) by choosing \( \pi'  \in \Pi\) s.t. 
\[
    \pi'(a^* | s) = 1 \text{ for some } a^* \in \argmax_{a \in A(s)} Q^\pi(s|a).
\]

This implies
\[
    \sum_{ a \in A}\pi(a|s) Q^\pi(s|a) \leq \sum_{a \in A} \pi'(a|s) \cdot Q^\pi(s|a),
\]

i.e. that \( \pi' \) is better than \( \pi \). We say that a \( \pi' \) chosen this way is \emph{greedy} w.r.t. \( Q^\pi \).
More generally:

\begin{definition}[\( \epsilon-greedy \)]
    Let \( q: S \times A \to \mathbb{R}, \epsilon \in [0,1] \). A policy \( \pi \) is called 

    \begin{enumerate}
        \item \( \epsilon \)\emph{-greedy} w.r.t. \( q \) iff for each \( s \in S \)
        \[
            \pi(a|s) = \begin{cases}
                (1-\epsilon) + \frac{\epsilon}{|A(s)|}, \text{ if } a=a^*(s)\\
                \frac{\epsilon}{|A(s)|} \text{ if } a  A(s) \setminus \left\{ a^*(s) \right\}\\
            \end{cases}
        \]
        for some \( a^*(s) \in \argmax_{ a \in A(s)} \left[ q(s,a) \right] \).
        In case of \( \epsilon> 0 \), we simply speak of \( \pi \) beeing \emph{greedy} w.r.t. q.
        \item \( \epsilon \)\emph{-soft}, iff
        \[
            \fa{a \in A(s), s \in S:} \pi(a|s) \geq \frac{\epsilon}{|A(s)|}.
        \]
    \end{enumerate}
\end{definition}

Formulated slightly differently, an \( \epsilon \)-greedy policy chooses an optimal action w.r.t. \( q \) with probability \( 1-\epsilon \) and a random action \note{The random action might give us the optimal action aswell in this formulation!} chosen uniformly from \( A(s) \) with prob. \( \epsilon \).

\begin{corollary}[Greedy policy improvement]
         Let \( \pi \in \Pi \) and choose \( \pi' \in \Pi \) greedy w.r.t \( Q^\pi \). Then 
        \[
            V^\pi \leq V^{\pi'}.
        \]
\end{corollary}






















\end{document}