% !TeX root = main.tex
\input{template/layout.tex}
\input{template/math.tex}
\input{course_specific.tex}





\title{Mathematics of Reinforcement Learning}

\author{Notes by Moritz Roos}
\date{}
%remove page number from tableofcontents page
%\AtBeginDocument{\addtocontents{toc}{\protect\thispagestyle{empty}}}
\begin{document}
\maketitle

\tableofcontents


\clearpage
\section{Introduction}
\textbf{Goals:}
\begin{itemize}
    \item Look at different types of problems.
    \item Learn the basic principles of Reinforcement Learning (RL).
    \item Lean when to apply RL and when not to?
\end{itemize}

\subsection{It's-a me, Mario!}
We want to fix some terminology by the example of the Super Mario game.
\begin{itemize}
    \item \emph{Agent:} player
    \item \emph{Environment:} game
    \item \emph{state:} the frame/screen being presented to the agent
    \item \emph{Action:} input on the controller
    \item \emph{policy:} a correspondence between states and actions
    \item \emph{episode:} one run of the game from start to finish
    \item \emph{reward:} feedback mechanism telling you how good/bad an action performs in a given state
    \item \emph{return:} criterion to be optimized, typically cumulative (discounted, expected) rewards
    \item \emph{state dynamics:} how the next state is obtained from the current state and the current action
\end{itemize}


\begin{figure}[ht]
    \centering
    %\caption{}
    %\incsvg{path/}{path/file}
    \incsvg{figures}{figures/examplemodell}
    \label{fig:examplemodell}
\end{figure}




\section{Markov decision processes}

\textbf{Goals:}
\begin{itemize}
    \item Develop a mathematical framework for dynamic decision making problems under uncertainty.
    \item Lean how to solve these problems if the model is known.
\end{itemize}

\subsection{Definition of markov decision processes}
We look for stochastic processes (a family of random variables indexed by time) \( \{S_n\}_{n \in  \mathbb{N}_0 }, \{A_n\}_{n \in \mathbb{N}_{0}} \{R_n\}_{n \in \mathbb{N}_{0}} \) modelling the dynamic evolution of states, actions and rewards.

\begin{definition}[Markov decision model]
    A \emph{Markov Decision Model} is a tuple \( (S, A, D, p, r ,\gamma) \) consisting of the following components:
    \begin{enumerate}
        \item a finite set \( S \)  called \emph{state space}.
        \item a finite set \( A \) called \emph{action space}
        \item a set \( D \subseteq S \times A \) whose elements are the \emph{admissible state-action pairs}
        \item a \emph{transition probability function} \( p: S \times S \times A \implies [0,1] \)
              \[
                  (s', s, a) \mapsto p(s' | s, a)\note{This is the probability of ending up in \( s' \) when performing action \( a \) in state \( s \). \( s' \mapsto p(s'| s,a) \) is a probability mass function.}
              \] satisfying
              \[
                  \sum_{s' \in S}p(s' | s, a) = 1
              \] for all \( (s,a) \in S \times A \).
        \item a \emph{reward function} \( r: D \to \mathbb{R} \)
              \[
                  (s,a) \mapsto r(s,a)\note{The reward for performing action \( a \)  in state \( s \).}
              \]
        \item a \emph{discount factor} \( \gamma \in (0,1] \).\note{This encodes the time-value of rewards. \( \gamma^{-1}\cdot r(s,a) \) is the value at time \( 0 \) of receiving \( r(s,a) \) \( n \) steps into the future.}

    \end{enumerate}
\end{definition}


\begin{definition}[Policy \qquad \( \Pi \)  \qquad \( \Pi_d \) ]\label{def_policy}
    A \emph{policy} is a mapping \( \pi: S \times A \to [0,1] \)
    \[
        (s,a) \mapsto \pi(a|s)
    \]
    such that
    \[
        \sum_{a \in A} \pi(a|s) = 1 \note{This means that \( a \mapsto \pi(a|s) \) is a probability mass function for each fixed \( s \) . Thus one action \textit{has} to be chosen.}
    \] for all \( s \in S \) and
    \[
        \pi(a|s) = 0
    \] for all \( s,a \in S\times A \setminus D\).

    We say that \( \pi \) is a \emph{deterministic policy} if
    \[
        \fa{s \in S:}\ex{a \in A:}\pi(a|s) = 1.
    \] We write \( \Pi \) and \( \Pi_d \) for the set of all policies and the subset of deterministic policies respectively.

\end{definition}

To wit, \( \pi(a|s) \) is interpreted as the probability of choosing action \( a \) in state \( s \). One could also introduce \quotes{non-Markovian} policies which depend on the entire history of states and actions or \quotes{non-stationary} policies which depend on the current state and current time.
However, we spare ourselves the trouble, since we will eventually see that there always is a \quotes{stationary, Markovian} optimal policy as in \cref{def_policy}.


\ \\
Intuitively, what happens for a given MDM and a policy \( \Pi \) is the following:

\begin{itemize}
    \item Start in an initial state \( S_{0}:= s_{0} \).
    \item Using the policy \( \pi \), randomly draw an action \( A_{0} \) from \( a \mapsto \pi(a|S_{0}) \).
    \item Collect the reward \( R_{0} := r(S_{0}, A_{0})\) and draw the next state \( S_{1} \) from \( s' \mapsto p(s'|S_{0}, A_{0}) \).
    \item repeat this procedure to construct \( S_{0}, A_{0}, R_{0}, S_{1}, A_{1}, R_{1},\mydots \)
\end{itemize}

Informally, we can then introduce the objective
\[
    \mathbb{E} \left[ \sum_{n=0}^{\infty} \gamma^{n} \cdot R_n \right] = \mathbb{E} \left[ \sum_{n=0}^{\infty} \gamma^{n} \cdot r(S_{n}, A_{n}) \right].
\]


Now we want to construct the mentioned stochastic processes. They should be consistent with our definition of MDM's and policies.\note{This can be seen as the formalization of the intuitive description of what should happen for a given MDM and a policy.} This enables us to make sense of notions like the expected return mentioned above.

For this, the first ingredient is a probability space. We choose the sample space as 
\[
    \Omega := (S \times A)^\infty = \bigtimes_{n=0}^{\infty} (S \times A)
\]
and let the \( \sigma \)-algebra \( \mathcal{A} \) on \( \Omega \) be the power set \( \mathcal{P}(\Omega) \) of \( \Omega \). We can safely do so, since \( \Omega \) is countable. In fact, any element \( w \in \Omega  \) takes the form 
\[
    \omega = (s_{0},a_{0},s_{1},a_{1},s_{2},a_{2},\mydots)
\]  
where \(  \left\{ s_n \right\}_{n \in \mathbb{N}_{0}} \subseteq S, \left\{ a_n \right\}_{n \in \mathbb{N}_{0}} \subseteq A \). With that in mind, we define functions
\[
    S_n(\omega) = S_n((s_{0},a_{0},s_{1},a_{1},\mydots)) := s_n
\]
\[
    A_n(\omega) = A_n((s_{0},a_{0},s_{1},a_{1},\mydots)) := a_n  
\]
for \( n \in \mathbb{N}_{0}, \omega \in \Omega \). 

Then \( S_n, A_n \) trivially\note{Trivially, since we chose the power set as \( \sigma \)-algebra. } are measurable functions ( = random variables) taking values in \( S, A \) respectively.  What is left is showing that there exists a probability measure \( \mathbb{P}^\pi_\mu \) on \( (\Omega, \mathcal{A}) \) such that 
\( S_{0},A_{0}, S_{1},A_{1},\mydots \) have the desired distribution.\note{The \( A \) 's should be determined by the policy and the last state. The \( S \) 's by the transition function and the taken action.}
This can be constructed from \( p, \pi \)  and an initial distribution \( \mu \) on \( S \) such that \( S_{0} \sim \mu \).
\end{document}